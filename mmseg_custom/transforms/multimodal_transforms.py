"""å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒé¢„å¤„ç†ç®¡é“

æ”¯æŒå…‰å­¦ã€SARã€çº¢å¤–ç­‰ä¸åŒæ¨¡æ€çš„å›¾åƒé¢„å¤„ç†å’Œæ•°æ®å¢å¼ºã€‚
é’ˆå¯¹MMRS-1Mæ•°æ®é›†çš„ç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–ã€‚
"""

import cv2
import numpy as np
from typing import Dict, List, Optional, Tuple, Union

import mmcv
from mmengine.registry import TRANSFORMS
from mmengine.structures import PixelData

# åŸºç¡€å˜æ¢ç±»
class BaseTransform:
    """åŸºç¡€å˜æ¢ç±»"""
    def __call__(self, results):
        return self.transform(results)
    
    def transform(self, results):
        raise NotImplementedError


@TRANSFORMS.register_module()
class MultiModalNormalize(BaseTransform):
    """å¤šæ¨¡æ€å›¾åƒå½’ä¸€åŒ–ã€‚
    
    é’ˆå¯¹ä¸åŒæ¨¡æ€ï¼ˆå…‰å­¦ã€SARã€çº¢å¤–ï¼‰ä½¿ç”¨ä¸åŒçš„å½’ä¸€åŒ–å‚æ•°ã€‚
    """
    
    def __init__(self,
                 modality: str = 'optical',
                 mean: Optional[List[float]] = None,
                 std: Optional[List[float]] = None,
                 to_rgb: bool = True):
        """åˆå§‹åŒ–å¤šæ¨¡æ€å½’ä¸€åŒ–ã€‚
        
        Args:
            modality (str): å›¾åƒæ¨¡æ€ç±»å‹
            mean (List[float], optional): å‡å€¼ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            std (List[float], optional): æ ‡å‡†å·®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            to_rgb (bool): æ˜¯å¦è½¬æ¢ä¸ºRGBæ ¼å¼
        """
        self.modality = modality
        self.to_rgb = to_rgb
        
        # ä¸åŒæ¨¡æ€çš„é»˜è®¤å½’ä¸€åŒ–å‚æ•°
        self.modality_params = {
            'optical': {
                'mean': [123.675, 116.28, 103.53],  # ImageNetç»Ÿè®¡å€¼
                'std': [58.395, 57.12, 57.375]
            },
            'sar': {
                'mean': [127.5],  # SARå›¾åƒé€šå¸¸æ˜¯å•é€šé“
                'std': [127.5]
            },
            'infrared': {
                'mean': [127.5, 127.5, 127.5],  # çº¢å¤–å›¾åƒ
                'std': [127.5, 127.5, 127.5]
            }
        }
        
        # ä½¿ç”¨æä¾›çš„å‚æ•°æˆ–é»˜è®¤å‚æ•°
        if mean is not None:
            self.mean = np.array(mean, dtype=np.float32)
        else:
            self.mean = np.array(self.modality_params[modality]['mean'], dtype=np.float32)
            
        if std is not None:
            self.std = np.array(std, dtype=np.float32)
        else:
            self.std = np.array(self.modality_params[modality]['std'], dtype=np.float32)
    
    def transform(self, results: Dict) -> Dict:
        """æ‰§è¡Œå½’ä¸€åŒ–å˜æ¢ã€‚"""
        img = results['img']
        
        # BGRè½¬RGB
        if self.to_rgb and img.shape[2] == 3:
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        # å½’ä¸€åŒ–
        img = img.astype(np.float32)
        
        # å¤„ç†ä¸åŒé€šé“æ•°çš„æƒ…å†µ
        if len(self.mean) == 1 and img.shape[2] == 3:
            # SARå•é€šé“å‚æ•°åº”ç”¨åˆ°ä¸‰é€šé“å›¾åƒ
            mean = np.array([self.mean[0]] * 3)
            std = np.array([self.std[0]] * 3)
        elif len(self.mean) == 3 and img.shape[2] == 1:
            # ä¸‰é€šé“å‚æ•°åº”ç”¨åˆ°å•é€šé“å›¾åƒ
            mean = np.array([np.mean(self.mean)])
            std = np.array([np.mean(self.std)])
        else:
            mean = self.mean
            std = self.std
        
        img = (img - mean) / std
        
        results['img'] = img
        results['img_norm_cfg'] = dict(
            mean=mean.tolist(),
            std=std.tolist(),
            to_rgb=self.to_rgb
        )
        
        return results


@TRANSFORMS.register_module()
class MultiModalResize(BaseTransform):
    """å¤šæ¨¡æ€å›¾åƒç¼©æ”¾ã€‚
    
    é’ˆå¯¹ä¸åŒæ¨¡æ€ä½¿ç”¨ä¸åŒçš„æ’å€¼æ–¹æ³•ã€‚
    """
    
    def __init__(self,
                 scale: Union[int, Tuple[int, int]],
                 modality: str = 'optical',
                 keep_ratio: bool = True):
        """åˆå§‹åŒ–å¤šæ¨¡æ€ç¼©æ”¾ã€‚
        
        Args:
            scale: ç›®æ ‡å°ºå¯¸
            modality: å›¾åƒæ¨¡æ€
            keep_ratio: æ˜¯å¦ä¿æŒå®½é«˜æ¯”
        """
        self.scale = scale if isinstance(scale, tuple) else (scale, scale)
        self.modality = modality
        self.keep_ratio = keep_ratio
        
        # ä¸åŒæ¨¡æ€çš„æ’å€¼æ–¹æ³•
        self.interpolation_methods = {
            'optical': cv2.INTER_LINEAR,
            'sar': cv2.INTER_NEAREST,  # SARå›¾åƒä½¿ç”¨æœ€è¿‘é‚»æ’å€¼ä¿æŒçº¹ç†
            'infrared': cv2.INTER_LINEAR
        }
    
    def transform(self, results: Dict) -> Dict:
        """æ‰§è¡Œç¼©æ”¾å˜æ¢ã€‚"""
        img = results['img']
        h, w = img.shape[:2]
        
        # è®¡ç®—æ–°å°ºå¯¸
        if self.keep_ratio:
            scale_factor = min(self.scale[0] / w, self.scale[1] / h)
            new_w = int(w * scale_factor)
            new_h = int(h * scale_factor)
        else:
            new_w, new_h = self.scale
        
        # é€‰æ‹©æ’å€¼æ–¹æ³•
        interpolation = self.interpolation_methods.get(self.modality, cv2.INTER_LINEAR)
        
        # ç¼©æ”¾å›¾åƒ
        img = cv2.resize(img, (new_w, new_h), interpolation=interpolation)
        
        results['img'] = img
        results['img_shape'] = img.shape
        results['scale_factor'] = (new_w / w, new_h / h)
        
        # å¦‚æœæœ‰åˆ†å‰²æ ‡æ³¨ï¼Œä¹Ÿéœ€è¦ç¼©æ”¾
        if 'gt_seg_map' in results:
            gt_seg_map = results['gt_seg_map']
            gt_seg_map = cv2.resize(gt_seg_map, (new_w, new_h), interpolation=cv2.INTER_NEAREST)
            results['gt_seg_map'] = gt_seg_map
        
        return results


@TRANSFORMS.register_module()
class SARSpecificAugmentation(BaseTransform):
    """SARå›¾åƒç‰¹å®šçš„æ•°æ®å¢å¼ºã€‚
    
    åŒ…æ‹¬æ–‘ç‚¹å™ªå£°æ¨¡æ‹Ÿã€å¯¹æ¯”åº¦å¢å¼ºç­‰ã€‚
    """
    
    def __init__(self,
                 speckle_prob: float = 0.3,
                 speckle_strength: float = 0.1,
                 contrast_prob: float = 0.5,
                 contrast_range: Tuple[float, float] = (0.8, 1.2)):
        """åˆå§‹åŒ–SARå¢å¼ºã€‚
        
        Args:
            speckle_prob: æ–‘ç‚¹å™ªå£°æ¦‚ç‡
            speckle_strength: æ–‘ç‚¹å™ªå£°å¼ºåº¦
            contrast_prob: å¯¹æ¯”åº¦è°ƒæ•´æ¦‚ç‡
            contrast_range: å¯¹æ¯”åº¦è°ƒæ•´èŒƒå›´
        """
        self.speckle_prob = speckle_prob
        self.speckle_strength = speckle_strength
        self.contrast_prob = contrast_prob
        self.contrast_range = contrast_range
    
    def transform(self, results: Dict) -> Dict:
        """æ‰§è¡ŒSARç‰¹å®šå¢å¼ºã€‚"""
        img = results['img']
        
        # æ–‘ç‚¹å™ªå£°
        if np.random.random() < self.speckle_prob:
            noise = np.random.gamma(1.0, self.speckle_strength, img.shape)
            img = img * noise
        
        # å¯¹æ¯”åº¦è°ƒæ•´
        if np.random.random() < self.contrast_prob:
            contrast_factor = np.random.uniform(*self.contrast_range)
            img = img * contrast_factor
        
        # ç¡®ä¿åƒç´ å€¼åœ¨åˆç†èŒƒå›´å†…
        img = np.clip(img, 0, 255)
        
        results['img'] = img.astype(np.uint8)
        
        return results


@TRANSFORMS.register_module()
class InfraredSpecificAugmentation(BaseTransform):
    """çº¢å¤–å›¾åƒç‰¹å®šçš„æ•°æ®å¢å¼ºã€‚
    
    åŒ…æ‹¬çƒ­å™ªå£°æ¨¡æ‹Ÿã€æ¸©åº¦èŒƒå›´è°ƒæ•´ç­‰ã€‚
    """
    
    def __init__(self,
                 thermal_noise_prob: float = 0.3,
                 thermal_noise_std: float = 5.0,
                 temperature_shift_prob: float = 0.4,
                 temperature_shift_range: Tuple[float, float] = (-10, 10)):
        """åˆå§‹åŒ–çº¢å¤–å¢å¼ºã€‚
        
        Args:
            thermal_noise_prob: çƒ­å™ªå£°æ¦‚ç‡
            thermal_noise_std: çƒ­å™ªå£°æ ‡å‡†å·®
            temperature_shift_prob: æ¸©åº¦åç§»æ¦‚ç‡
            temperature_shift_range: æ¸©åº¦åç§»èŒƒå›´
        """
        self.thermal_noise_prob = thermal_noise_prob
        self.thermal_noise_std = thermal_noise_std
        self.temperature_shift_prob = temperature_shift_prob
        self.temperature_shift_range = temperature_shift_range
    
    def transform(self, results: Dict) -> Dict:
        """æ‰§è¡Œçº¢å¤–ç‰¹å®šå¢å¼ºã€‚"""
        img = results['img'].astype(np.float32)
        
        # çƒ­å™ªå£°
        if np.random.random() < self.thermal_noise_prob:
            noise = np.random.normal(0, self.thermal_noise_std, img.shape)
            img = img + noise
        
        # æ¸©åº¦åç§»
        if np.random.random() < self.temperature_shift_prob:
            shift = np.random.uniform(*self.temperature_shift_range)
            img = img + shift
        
        # ç¡®ä¿åƒç´ å€¼åœ¨åˆç†èŒƒå›´å†…
        img = np.clip(img, 0, 255)
        
        results['img'] = img.astype(np.uint8)
        
        return results


@TRANSFORMS.register_module()
class PhotoMetricDistortion(BaseTransform):
    """å…‰åº¦å¤±çœŸå˜æ¢ã€‚
    
    å¯¹å›¾åƒè¿›è¡Œäº®åº¦ã€å¯¹æ¯”åº¦ã€é¥±å’Œåº¦å’Œè‰²è°ƒçš„éšæœºè°ƒæ•´ã€‚
    """
    
    def __init__(self,
                 brightness_delta: int = 32,
                 contrast_range: Tuple[float, float] = (0.5, 1.5),
                 saturation_range: Tuple[float, float] = (0.5, 1.5),
                 hue_delta: int = 18):
        """åˆå§‹åŒ–å…‰åº¦å¤±çœŸå˜æ¢ã€‚
        
        Args:
            brightness_delta: äº®åº¦è°ƒæ•´èŒƒå›´
            contrast_range: å¯¹æ¯”åº¦è°ƒæ•´èŒƒå›´
            saturation_range: é¥±å’Œåº¦è°ƒæ•´èŒƒå›´
            hue_delta: è‰²è°ƒè°ƒæ•´èŒƒå›´
        """
        self.brightness_delta = brightness_delta
        self.contrast_range = contrast_range
        self.saturation_range = saturation_range
        self.hue_delta = hue_delta

    def transform(self, results: Dict) -> Dict:
        """æ‰§è¡Œå…‰åº¦å¤±çœŸå˜æ¢ã€‚"""
        img = results['img'].astype(np.float32)
        
        # éšæœºäº®åº¦è°ƒæ•´
        if np.random.randint(2):
            delta = np.random.uniform(-self.brightness_delta, self.brightness_delta)
            img += delta
            
        # éšæœºå¯¹æ¯”åº¦è°ƒæ•´
        if np.random.randint(2):
            alpha = np.random.uniform(*self.contrast_range)
            img *= alpha
            
        # éšæœºé¥±å’Œåº¦è°ƒæ•´ï¼ˆä»…å¯¹RGBå›¾åƒï¼‰
        if len(img.shape) == 3 and img.shape[2] == 3:
            if np.random.randint(2):
                # è½¬æ¢åˆ°HSVç©ºé—´è¿›è¡Œé¥±å’Œåº¦è°ƒæ•´
                img_hsv = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2HSV).astype(np.float32)
                alpha = np.random.uniform(*self.saturation_range)
                img_hsv[:, :, 1] *= alpha
                img_hsv[:, :, 1] = np.clip(img_hsv[:, :, 1], 0, 255)
                img = cv2.cvtColor(img_hsv.astype(np.uint8), cv2.COLOR_HSV2RGB).astype(np.float32)
            
            # éšæœºè‰²è°ƒè°ƒæ•´
            if np.random.randint(2):
                img_hsv = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2HSV).astype(np.float32)
                delta = np.random.uniform(-self.hue_delta, self.hue_delta)
                img_hsv[:, :, 0] += delta
                img_hsv[:, :, 0] = np.clip(img_hsv[:, :, 0], 0, 179)  # OpenCVä¸­Hé€šé“èŒƒå›´æ˜¯0-179
                img = cv2.cvtColor(img_hsv.astype(np.uint8), cv2.COLOR_HSV2RGB).astype(np.float32)
        
        # ç¡®ä¿åƒç´ å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        img = np.clip(img, 0, 255).astype(np.uint8)
        results['img'] = img
        
        return results


@TRANSFORMS.register_module()
class PackSegInputs:
    """Pack the inputs data for the semantic segmentation.
    
    This transform packs the image and ground truth segmentation map into
    a format that can be consumed by the model. It returns the data in the
    standard MMEngine format expected by pseudo_collate.
    """
    
    def __init__(self, meta_keys=('img_path', 'seg_map_path', 'ori_shape', 
                                  'img_shape', 'pad_shape', 'scale_factor', 
                                  'flip', 'flip_direction')):
        """Initialize PackSegInputs.
        
        Args:
            meta_keys (tuple): Keys to be packed into meta information.
        """
        self.meta_keys = meta_keys

    def __call__(self, results):
        """Pack the inputs data.
        
        Args:
            results (dict): Result dict from loading pipeline.
            
        Returns:
            dict: Results with 'inputs' key in standard MMEngine format.
        """
        import torch
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåˆ›å»ºæ ‡å‡†çš„ 'inputs' é”®
        if 'img' in results:
            img = results['img']
            
            # ç¡®ä¿å›¾åƒæ˜¯torch.Tensoræ ¼å¼
            if isinstance(img, np.ndarray):
                # è½¬æ¢numpyæ•°ç»„ä¸ºtorch tensor
                img = torch.from_numpy(img.copy()).float()
                
            # ç¡®ä¿å›¾åƒæ˜¯CHWæ ¼å¼
            if len(img.shape) == 3:
                # å¦‚æœæ˜¯HWCæ ¼å¼ï¼Œè½¬æ¢ä¸ºCHW
                if img.shape[2] == 3:  # HWC
                    img = img.permute(2, 0, 1)  # è½¬æ¢ä¸ºCHW
                    
            # åˆ›å»ºæ ‡å‡†çš„ 'inputs' é”®ä¾›æ¨¡å‹ä½¿ç”¨
            results['inputs'] = img
            # ä¿ç•™åŸå§‹çš„ 'img' é”®ä»¥å…¼å®¹å…¶ä»–ç»„ä»¶
            results['img'] = img
            
        # ç¡®ä¿åˆ†å‰²å›¾æ ¼å¼æ­£ç¡®
        if 'gt_seg_map' in results:
            gt_seg_map = results['gt_seg_map']
            # ç¡®ä¿åˆ†å‰²å›¾æ˜¯2Dçš„
            if len(gt_seg_map.shape) == 3:
                gt_seg_map = gt_seg_map.squeeze()
            results['gt_semantic_seg'] = gt_seg_map
            
        # æ·»åŠ å¿…è¦çš„å­—æ®µ
        if 'seg_fields' not in results:
            results['seg_fields'] = []
        if 'gt_semantic_seg' in results and 'gt_semantic_seg' not in results['seg_fields']:
            results['seg_fields'].append('gt_semantic_seg')
            
        # ç¡®ä¿img_fieldså­˜åœ¨
        if 'img_fields' not in results:
            results['img_fields'] = ['img']
            
        return results


@TRANSFORMS.register_module()
class RandomCrop(BaseTransform):
    """æ ‡å‡†éšæœºè£å‰ªå˜æ¢ã€‚
    
    å…¼å®¹mmsegçš„RandomCropæ¥å£ã€‚
    """
    
    def __init__(self,
                 crop_size: Union[int, Tuple[int, int]],
                 cat_max_ratio: float = 1.0,
                 ignore_index: int = 255):
        """åˆå§‹åŒ–éšæœºè£å‰ªã€‚
        
        Args:
            crop_size: è£å‰ªå°ºå¯¸
            cat_max_ratio: ç±»åˆ«æœ€å¤§æ¯”ä¾‹
            ignore_index: å¿½ç•¥ç´¢å¼•
        """
        self.crop_size = crop_size if isinstance(crop_size, tuple) else (crop_size, crop_size)
        self.cat_max_ratio = cat_max_ratio
        self.ignore_index = ignore_index

    def transform(self, results: Dict) -> Dict:
        """æ‰§è¡Œéšæœºè£å‰ªã€‚"""
        img = results['img']
        h, w = img.shape[:2]
        crop_h, crop_w = self.crop_size
        
        # å¦‚æœå›¾åƒå°äºè£å‰ªå°ºå¯¸ï¼Œå…ˆè¿›è¡Œå¡«å……
        if h < crop_h or w < crop_w:
            pad_h = max(0, crop_h - h)
            pad_w = max(0, crop_w - w)
            
            # ä½¿ç”¨ImageNetå‡å€¼å¡«å……
            pad_value = [123.675, 116.28, 103.53]
            
            img = cv2.copyMakeBorder(
                img, 0, pad_h, 0, pad_w, 
                cv2.BORDER_CONSTANT, 
                value=pad_value[:img.shape[2]]
            )
            
            if 'gt_seg_map' in results:
                gt_seg_map = results['gt_seg_map']
                gt_seg_map = cv2.copyMakeBorder(
                    gt_seg_map, 0, pad_h, 0, pad_w,
                    cv2.BORDER_CONSTANT,
                    value=self.ignore_index
                )
                results['gt_seg_map'] = gt_seg_map
            
            h, w = img.shape[:2]
        
        # éšæœºé€‰æ‹©è£å‰ªä½ç½®
        top = np.random.randint(0, h - crop_h + 1)
        left = np.random.randint(0, w - crop_w + 1)
        
        # æ‰§è¡Œè£å‰ª
        results['img'] = img[top:top + crop_h, left:left + crop_w]
        
        if 'gt_seg_map' in results:
            results['gt_seg_map'] = results['gt_seg_map'][top:top + crop_h, left:left + crop_w]
        
        # æ›´æ–°å›¾åƒå½¢çŠ¶ä¿¡æ¯
        results['img_shape'] = (crop_h, crop_w)
        
        return results


@TRANSFORMS.register_module()
class MultiModalRandomCrop(BaseTransform):
    """å¤šæ¨¡æ€éšæœºè£å‰ªã€‚
    
    é’ˆå¯¹ä¸åŒæ¨¡æ€çš„ç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–çš„éšæœºè£å‰ªã€‚
    """
    
    def __init__(self,
                 crop_size: Union[int, Tuple[int, int]],
                 modality: str = 'optical',
                 cat_max_ratio: float = 1.0,
                 ignore_index: int = 255):
        """åˆå§‹åŒ–å¤šæ¨¡æ€éšæœºè£å‰ªã€‚
        
        Args:
            crop_size: è£å‰ªå°ºå¯¸
            modality: å›¾åƒæ¨¡æ€
            cat_max_ratio: ç±»åˆ«æœ€å¤§æ¯”ä¾‹
            ignore_index: å¿½ç•¥ç´¢å¼•
        """
        self.crop_size = crop_size if isinstance(crop_size, tuple) else (crop_size, crop_size)
        self.modality = modality
        self.cat_max_ratio = cat_max_ratio
        self.ignore_index = ignore_index
    
    def transform(self, results: Dict) -> Dict:
        """æ‰§è¡Œéšæœºè£å‰ªã€‚"""
        img = results['img']
        h, w = img.shape[:2]
        crop_h, crop_w = self.crop_size
        
        # å¦‚æœå›¾åƒå°äºè£å‰ªå°ºå¯¸ï¼Œå…ˆè¿›è¡Œå¡«å……
        if h < crop_h or w < crop_w:
            pad_h = max(0, crop_h - h)
            pad_w = max(0, crop_w - w)
            
            # ä¸åŒæ¨¡æ€ä½¿ç”¨ä¸åŒçš„å¡«å……å€¼
            if self.modality == 'optical':
                pad_value = [123.675, 116.28, 103.53]  # ImageNetå‡å€¼
            elif self.modality == 'sar':
                pad_value = [0]  # SARå›¾åƒç”¨0å¡«å……
            else:  # infrared
                pad_value = [127.5, 127.5, 127.5]  # ä¸­æ€§ç°åº¦å€¼
            
            img = cv2.copyMakeBorder(
                img, 0, pad_h, 0, pad_w, 
                cv2.BORDER_CONSTANT, 
                value=pad_value[:img.shape[2]]
            )
            
            if 'gt_seg_map' in results:
                gt_seg_map = results['gt_seg_map']
                gt_seg_map = cv2.copyMakeBorder(
                    gt_seg_map, 0, pad_h, 0, pad_w,
                    cv2.BORDER_CONSTANT,
                    value=self.ignore_index
                )
                results['gt_seg_map'] = gt_seg_map
            
            h, w = img.shape[:2]
        
        # éšæœºé€‰æ‹©è£å‰ªä½ç½®
        top = np.random.randint(0, h - crop_h + 1)
        left = np.random.randint(0, w - crop_w + 1)
        
        # æ‰§è¡Œè£å‰ª
        img = img[top:top + crop_h, left:left + crop_w]
        results['img'] = img
        results['img_shape'] = img.shape
        
        if 'gt_seg_map' in results:
            gt_seg_map = results['gt_seg_map']
            gt_seg_map = gt_seg_map[top:top + crop_h, left:left + crop_w]
            results['gt_seg_map'] = gt_seg_map
        
        return results


def build_multimodal_pipeline(modality: str = 'optical',
                             crop_size: Tuple[int, int] = (512, 512),
                             training: bool = True) -> List[Dict]:
    """æ„å»ºå¤šæ¨¡æ€é¢„å¤„ç†ç®¡é“ã€‚
    
    Args:
        modality: å›¾åƒæ¨¡æ€ç±»å‹
        crop_size: è£å‰ªå°ºå¯¸
        training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
    
    Returns:
        é¢„å¤„ç†ç®¡é“é…ç½®åˆ—è¡¨
    """
    pipeline = []
    
    # åŸºç¡€å˜æ¢
    pipeline.extend([
        dict(type='LoadImageFromFile'),
        dict(type='LoadAnnotations') if training else dict(type='LoadAnnotations', reduce_zero_label=True),
    ])
    
    # å¤šæ¨¡æ€ç¼©æ”¾
    pipeline.append(
        dict(
            type='MultiModalResize',
            scale=crop_size,
            modality=modality,
            keep_ratio=True
        )
    )
    
    if training:
        # è®­ç»ƒæ—¶çš„æ•°æ®å¢å¼º
        pipeline.extend([
            dict(
                type='MultiModalRandomCrop',
                crop_size=crop_size,
                modality=modality
            ),
            dict(type='RandomFlip', prob=0.5),
        ])
        
        # æ¨¡æ€ç‰¹å®šå¢å¼º
        if modality == 'sar':
            pipeline.append(
                dict(
                    type='SARSpecificAugmentation',
                    speckle_prob=0.3,
                    contrast_prob=0.5
                )
            )
        elif modality == 'infrared':
            pipeline.append(
                dict(
                    type='InfraredSpecificAugmentation',
                    thermal_noise_prob=0.3,
                    temperature_shift_prob=0.4
                )
            )
    
    # å½’ä¸€åŒ–
    pipeline.append(
        dict(
            type='MultiModalNormalize',
            modality=modality,
            to_rgb=True
        )
    )
    
    # æ‰“åŒ…è¾“å…¥
    pipeline.append(
        dict(
            type='PackSegInputs',
            meta_keys=('img_path', 'seg_map_path', 'ori_shape', 'img_shape', 
                      'pad_shape', 'scale_factor', 'flip', 'flip_direction',
                      'modality', 'task_type')
        )
    )
    
    return pipeline