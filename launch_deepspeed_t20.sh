#!/bin/bash

# ç‡§åŽŸT20 DeepSpeedè®­ç»ƒå¯åŠ¨è„šæœ¬ (åŸºäºŽå®˜æ–¹æœ€ä½³å®žè·µä¼˜åŒ–ç‰ˆ)
# ä¸¥æ ¼éµå¾ªç‡§åŽŸå®˜æ–¹åˆ†å¸ƒå¼è®­ç»ƒè§„èŒƒ

set -eu -o pipefail

# ðŸ”§ ç‡§åŽŸGCUæ ¸å¿ƒçŽ¯å¢ƒå˜é‡é…ç½® - åŸºäºŽå®˜æ–¹æœ€ä½³å®žè·µ
export ENFLAME_CLUSTER_PARALLEL=true
export ENFLAME_ENABLE_EFP=true
export TOPS_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# ðŸ”§ ç‡§åŽŸåˆ†å¸ƒå¼è®­ç»ƒçŽ¯å¢ƒå˜é‡ - å®˜æ–¹æŽ¨èé…ç½®
export OMP_NUM_THREADS=5
export ECCL_ASYNC_DISABLE=false
export ENABLE_RDMA=true
export ECCL_MAX_NCHANNELS=2
export ENFLAME_UMD_FLAGS="mem_alloc_retry_times=1"
export ECCL_RUNTIME_3_0_ENABLE=true
export ENFLAME_PT_EVALUATE_TENSOR_NEEDED=false

# ðŸ”§ ç¦ç”¨CUDAç›¸å…³è®¾å¤‡ï¼Œå¼ºåˆ¶ä½¿ç”¨GCU
export CUDA_VISIBLE_DEVICES=""

# ðŸ”§ åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
export MASTER_ADDR=${MASTER_ADDR:-"localhost"}
export MASTER_PORT=${MASTER_PORT:-"29500"}
export GPUS_PER_NODE=8
export NNODES=1
export NODE_RANK=0

# ðŸ”§ DeepSpeed GCUå…¼å®¹æ€§çŽ¯å¢ƒå˜é‡ - ç¦ç”¨æ‰€æœ‰CUDAç‰¹å®šç»„ä»¶
export DS_BUILD_FUSED_ADAM=0
export DEEPSPEED_DISABLE_FUSED_ADAM=1
export DS_BUILD_CPU_ADAM=1
export DS_BUILD_UTILS=0
export DS_BUILD_AIO=0
export DS_BUILD_SPARSE_ATTN=0
export DS_BUILD_FUSED_LAMB=0
export DS_BUILD_TRANSFORMER=0

# è®­ç»ƒå‚æ•°
CONFIG_FILE="configs/train_dinov3_mmrs1m_t20_gcu_8card.py"
WORK_DIR="work_dirs/dinov3_mmrs1m_t20_8card"
DEEPSPEED_CONFIG="deepspeed_config.json"

# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p ${WORK_DIR}

# ðŸ”§ ç”Ÿæˆç‡§åŽŸGCUä¼˜åŒ–çš„DeepSpeedé…ç½®æ–‡ä»¶ - åŸºäºŽå®˜æ–¹æœ€ä½³å®žè·µ
cat > ${DEEPSPEED_CONFIG} << EOF
{
    "train_batch_size": 64,
    "train_micro_batch_size_per_gpu": 8,
    "gradient_accumulation_steps": 1,
    "steps_per_print": 100,
    
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 1e-4,
            "betas": [0.9, 0.999],
            "eps": 1e-8,
            "weight_decay": 0.05
        }
    },
    
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 100000,
            "warmup_num_steps": 1000,
            "warmup_max_lr": 1e-4,
            "warmup_min_lr": 1e-6
        }
    },
    
    "zero_optimization": {
        "stage": 0
    },
    
    "fp16": {
        "enabled": false
    },
    
    "bf16": {
        "enabled": false
    },
    
    "gradient_clipping": 1.0,
    "wall_clock_breakdown": true,
    "disable_fused_adam": true
}
EOF

echo "ðŸš€ å¯åŠ¨ç‡§åŽŸT20 8å¡åˆ†å¸ƒå¼DeepSpeedè®­ç»ƒ..."
echo "ðŸ“ é…ç½®æ–‡ä»¶: ${CONFIG_FILE}"
echo "ðŸ“ å·¥ä½œç›®å½•: ${WORK_DIR}"
echo "ðŸ“ DeepSpeedé…ç½®: ${DEEPSPEED_CONFIG}"
echo "ðŸ”§ ä½¿ç”¨ç‡§åŽŸå®˜æ–¹torch.distributed.launchæ–¹å¼å¯åŠ¨"

# ðŸ”§ ä½¿ç”¨ç‡§åŽŸå®˜æ–¹æŽ¨èçš„torch.distributed.launchå¯åŠ¨æ–¹å¼
# è¿™æ˜¯ç‡§åŽŸGCUåˆ†å¸ƒå¼è®­ç»ƒçš„æ ‡å‡†å¯åŠ¨æ–¹æ³•ï¼Œå‚è€ƒå®˜æ–¹llama2ç¤ºä¾‹
DISTRIBUTED_ARGS="--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT"

python3 -u -m torch.distributed.launch $DISTRIBUTED_ARGS \
    scripts/train_dinov3_deepspeed_8card_gcu.py \
    --config ${CONFIG_FILE} \
    --work-dir ${WORK_DIR} \
    --deepspeed ${DEEPSPEED_CONFIG} \
    --launcher pytorch \
    --distributed-backend eccl

echo "âœ… è®­ç»ƒå®Œæˆ!"