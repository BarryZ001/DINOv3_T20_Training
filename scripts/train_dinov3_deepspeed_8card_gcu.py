#!/usr/bin/env python3
"""
DINOv3 + MMRS-1M 8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ (åŸºäºDeepSpeed)
ä½¿ç”¨DeepSpeedæ¡†æ¶è¿›è¡ŒGCUç¯å¢ƒä¸‹çš„åˆ†å¸ƒå¼è®­ç»ƒ
"""
import argparse
import os
import sys
import time
import warnings
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è®¾ç½®GCUç¯å¢ƒå˜é‡
os.environ.setdefault('PYTORCH_GCU_ALLOC_CONF', 'backend:topsMallocAsync')
os.environ.setdefault('TORCH_ECCL_AVOID_RECORD_STREAMS', 'false')
os.environ.setdefault('TORCH_ECCL_ASYNC_ERROR_HANDLING', '3')

# å¯¼å…¥å¿…è¦çš„åº“
try:
    import torch
    print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    import torch_gcu  # ç‡§åŸGCUæ”¯æŒ
    print(f"âœ… torch_gcuå¯ç”¨: {torch_gcu.is_available()}")
    if torch_gcu.is_available():
        print(f"âœ… GCUè®¾å¤‡æ•°: {torch_gcu.device_count()}")
    else:
        raise RuntimeError("torch_gcuä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…")
    
    import deepspeed
    print(f"âœ… DeepSpeedç‰ˆæœ¬: {deepspeed.__version__}")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

try:
    from mmengine.config import Config
    from mmengine.registry import MODELS, DATASETS
    print("âœ… MMEngineå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ MMEngineå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

try:
    import mmseg
    from mmseg.models import *
    from mmseg.apis import init_segmentor
    print("âœ… MMSegmentationå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ MMSegmentationå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    import mmseg_custom.models
    import mmseg_custom.datasets  # è¿™ä¼šæ³¨å†ŒMMRS1MDatasetåˆ°MMEngineçš„DATASETS
    import mmseg_custom.transforms
    print("âœ… è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
except ImportError as e:
    print(f"âš ï¸ è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    # å°è¯•æ‰‹åŠ¨å¯¼å…¥å…³é”®ç»„ä»¶
    try:
        from mmseg_custom.datasets.mmrs1m_dataset import MMRS1MDataset
        from mmseg_custom.datasets.loveda_dataset import LoveDADataset
        print("âœ… æ‰‹åŠ¨å¯¼å…¥æ•°æ®é›†ç±»æˆåŠŸ")
    except ImportError as e2:
        print(f"âŒ æ‰‹åŠ¨å¯¼å…¥æ•°æ®é›†å¤±è´¥: {e2}")
        sys.exit(1)

def setup_gcu_environment():
    """è®¾ç½®GCUç¯å¢ƒ - å·²åºŸå¼ƒï¼Œä½¿ç”¨mainå‡½æ•°ä¸­çš„ç®€åŒ–ç‰ˆæœ¬"""
    # è¿™ä¸ªå‡½æ•°å·²è¢«åºŸå¼ƒï¼Œç°åœ¨ç›´æ¥åœ¨mainå‡½æ•°ä¸­ä½¿ç”¨ä¸æˆåŠŸdemoç›¸åŒçš„æ–¹å¼
    pass

def make_deepspeed_config(config_path="/tmp/ds_config.json"):
    """åˆ›å»ºDeepSpeedé…ç½®æ–‡ä»¶"""
    cfg = {
        "train_batch_size": 16,  # æ€»batch size
        "train_micro_batch_size_per_gpu": 2,  # æ¯ä¸ªGPUçš„micro batch size
        "gradient_accumulation_steps": 1,
        "fp16": {"enabled": False},  # GCUç¯å¢ƒä¸‹æš‚æ—¶ä¸ä½¿ç”¨fp16
        "zero_optimization": {"stage": 0},  # ä¸ä½¿ç”¨ZeROä¼˜åŒ–
        "steps_per_print": 10,
        "wall_clock_breakdown": False
    }
    
    with open(config_path, "w") as f:
        json.dump(cfg, f, indent=2)
    
    print(f"ğŸ“ DeepSpeedé…ç½®æ–‡ä»¶: {config_path}")
    return config_path

def load_and_validate_config(config_path, work_dir=None):
    """åŠ è½½å’ŒéªŒè¯é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    print(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    cfg = Config.fromfile(config_path)
    
    # è®¾ç½®å·¥ä½œç›®å½•
    if work_dir is not None:
        cfg.work_dir = work_dir
    elif cfg.get('work_dir', None) is None:
        cfg.work_dir = './work_dirs/dinov3_deepspeed_8card_gcu'
    
    print(f"ğŸ“ å·¥ä½œç›®å½•: {cfg.work_dir}")
    
    # ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
    os.makedirs(cfg.work_dir, exist_ok=True)
    os.makedirs(f"{cfg.work_dir}/logs", exist_ok=True)
    
    # éªŒè¯å…³é”®é…ç½®
    if not hasattr(cfg, 'model'):
        raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘modelé…ç½®")
    
    if not hasattr(cfg, 'train_dataloader'):
        raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘train_dataloaderé…ç½®")
    
    print("âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")
    return cfg

def custom_collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°ï¼Œå¤„ç†MMSegçš„DataContainerå¯¹è±¡å¹¶ç»Ÿä¸€æ•°æ®æ ¼å¼"""
    import torch
    from torch.utils.data.dataloader import default_collate
    
    # å¤„ç†DataContainerå¯¹è±¡
    def extract_data_from_container(item):
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯DataContainer
            if hasattr(item, 'data'):
                return item.data
            else:
                return item
        except:
            return item
    
    # é€’å½’å¤„ç†batchä¸­çš„æ¯ä¸ªå…ƒç´ 
    def process_batch_item(item):
        if isinstance(item, dict):
            return {key: process_batch_item(value) for key, value in item.items()}
        elif isinstance(item, (list, tuple)):
            return [process_batch_item(x) for x in item]
        else:
            return extract_data_from_container(item)
    
    # å¤„ç†æ•´ä¸ªbatch
    processed_batch = [process_batch_item(item) for item in batch]
    
    # æ£€æŸ¥å¹¶ç»Ÿä¸€æ•°æ®æ ¼å¼
    if processed_batch and isinstance(processed_batch[0], dict):
        # å¦‚æœbatchæ˜¯å­—å…¸åˆ—è¡¨ï¼Œéœ€è¦åˆå¹¶æˆç»Ÿä¸€æ ¼å¼
        collated_dict = {}
        
        # è·å–æ‰€æœ‰é”®
        all_keys = set()
        for item in processed_batch:
            if isinstance(item, dict):
                all_keys.update(item.keys())
        
        # å¯¹æ¯ä¸ªé”®è¿›è¡Œcollate
        for key in all_keys:
            values = []
            for item in processed_batch:
                if isinstance(item, dict) and key in item:
                    val = item[key]
                    # ç¡®ä¿å›¾åƒæ•°æ®æ˜¯tensoræ ¼å¼
                    if key in ['img', 'inputs'] and not isinstance(val, torch.Tensor):
                        if hasattr(val, 'data'):
                            val = val.data
                        if isinstance(val, (list, tuple)) and len(val) > 0:
                            # å¦‚æœæ˜¯list/tupleï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                            val = val[0] if isinstance(val[0], torch.Tensor) else torch.tensor(val[0])
                    values.append(val)
            
            # å¯¹valuesè¿›è¡Œcollate
            if values:
                try:
                    if key in ['img', 'inputs']:
                        # å¯¹å›¾åƒæ•°æ®è¿›è¡Œç‰¹æ®Šå¤„ç†ï¼Œç¡®ä¿å°ºå¯¸ä¸€è‡´
                        tensor_values = []
                        target_size = None
                        
                        for val in values:
                            if isinstance(val, torch.Tensor):
                                if target_size is None:
                                    target_size = val.shape[-2:]  # å–H, W
                                
                                # å¦‚æœå°ºå¯¸ä¸åŒ¹é…ï¼Œè¿›è¡Œresize
                                if val.shape[-2:] != target_size:
                                    # ç®€å•çš„resizeåˆ°ç›®æ ‡å°ºå¯¸
                                    import torch.nn.functional as F
                                    val = F.interpolate(val.unsqueeze(0), size=target_size, mode='bilinear', align_corners=False).squeeze(0)
                                
                                tensor_values.append(val)
                        
                        if tensor_values:
                            collated_dict[key] = torch.stack(tensor_values)
                    else:
                        collated_dict[key] = default_collate(values)
                except Exception as e:
                    print(f"âš ï¸ Collateé”® '{key}' å¤±è´¥: {e}")
                    # å¦‚æœcollateå¤±è´¥ï¼Œä¿æŒåŸå§‹æ ¼å¼
                    collated_dict[key] = values
        
        return collated_dict
    else:
        # ä½¿ç”¨é»˜è®¤çš„collateå‡½æ•°å¤„ç†å¤„ç†åçš„æ•°æ®
        try:
            return default_collate(processed_batch)
        except Exception as e:
            print(f"âš ï¸ Collateå¤±è´¥: {e}")
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè¿”å›åŸå§‹batch
            return processed_batch

def build_model_and_dataset(cfg, device_name):
    """æ„å»ºæ¨¡å‹å’Œæ•°æ®é›†"""
    print(f"ğŸ“Š æ„å»ºæ•°æ®é›†: {cfg.train_dataloader.dataset.type}")
    
    # ä½¿ç”¨MMEngineçš„ç»Ÿä¸€æ„å»ºå™¨æ„å»ºè®­ç»ƒæ•°æ®é›†
    train_dataset = DATASETS.build(cfg.train_dataloader.dataset)
    print(f"âœ… è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
    
    # æ„å»ºéªŒè¯æ•°æ®é›†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    val_dataset = None
    if hasattr(cfg, 'val_dataloader') and cfg.val_dataloader is not None:
        val_dataset = DATASETS.build(cfg.val_dataloader.dataset)
        print(f"âœ… éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset)}")
    
    # æ„å»ºæ¨¡å‹
    print(f"ğŸ—ï¸ æ„å»ºæ¨¡å‹: {cfg.model.type}")
    model = MODELS.build(cfg.model)
    print(f"âœ… æ¨¡å‹æ„å»ºå®Œæˆ")
    
    # è®¾ç½®è®¾å¤‡
    if device_name.startswith('xla'):
        device = torch_gcu.device(device_name)
        # å¯¹äºGCUè®¾å¤‡ï¼Œç›´æ¥ä½¿ç”¨device_nameå­—ç¬¦ä¸²
        model = model.to(device_name)
    else:
        device = torch.device(device_name)
        model = model.to(device)
    
    print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device_name}")
    
    return model, train_dataset, val_dataset

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå‚æ•°è§£æå™¨ï¼Œæ­£ç¡®å¤„ç†DeepSpeedçš„--local_rankå‚æ•°
    parser = argparse.ArgumentParser(description='DINOv3 + MMRS-1M 8å¡åˆ†å¸ƒå¼è®­ç»ƒ')
    parser.add_argument('--config', type=str, 
                       default="configs/train_dinov3_mmrs1m_t20_gcu_8card.py",
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--work-dir', type=str, default=None,
                       help='å·¥ä½œç›®å½•')
    parser.add_argument('--local_rank', type=int, default=-1,
                       help='DeepSpeedè‡ªåŠ¨æ·»åŠ çš„local rankå‚æ•°')
    parser.add_argument('--steps', type=int, default=1000,
                       help='è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    print(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {args.work_dir}")
    print(f"ğŸ“ Local Rank: {args.local_rank}")
    print(f"ğŸ“ è®­ç»ƒæ­¥æ•°: {args.steps}")

    print("ğŸš€ å¯åŠ¨DINOv3 + MMRS-1M 8å¡åˆ†å¸ƒå¼è®­ç»ƒ")
    print("=" * 60)
    
    # 1. è®¾ç½®GCUç¯å¢ƒ - ä½¿ç”¨ä¸æˆåŠŸdemoç›¸åŒçš„æ–¹å¼
    local_rank = args.local_rank if args.local_rank >= 0 else int(os.environ.get("LOCAL_RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", "1"))
    
    # è®¾ç½®è®¾å¤‡
    device_name = f"xla:{local_rank}"
    print(f"[PID {os.getpid()}] GCUç¯å¢ƒ - local_rank={local_rank}, world_size={world_size}, device={device_name}")
    
    # è®¾ç½®GCUè®¾å¤‡
    if torch_gcu is not None:
        torch_gcu.set_device(local_rank)
    else:
        print("âš ï¸ torch_gcuä¸å¯ç”¨ï¼Œè·³è¿‡è®¾å¤‡è®¾ç½®")
    
    # 2. åŠ è½½é…ç½®
    cfg = load_and_validate_config(args.config, args.work_dir)
    
    # æ„å»ºæ¨¡å‹å’Œæ•°æ®é›†
    model, train_dataset, val_dataset = build_model_and_dataset(cfg, device_name)
    

    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=cfg.train_dataloader.get('batch_size', 2),
        shuffle=True,
        num_workers=cfg.train_dataloader.get('num_workers', 2),
        pin_memory=False,  # GCUç¯å¢ƒä¸‹ä¸ä½¿ç”¨pin_memory
        collate_fn=custom_collate_fn  # ä½¿ç”¨è‡ªå®šä¹‰çš„collate_fnå¤„ç†DataContainer
    )
    
    # 5. åˆ›å»ºä¼˜åŒ–å™¨ - ä½¿ç”¨ä¸æˆåŠŸdemoç›¸åŒçš„Adamä¼˜åŒ–å™¨
    print("ğŸ”§ åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    print("âœ… ä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆ")
    
    # åˆ›å»ºDeepSpeedé…ç½®
    ds_config_path = make_deepspeed_config()
    
    # 6. åˆå§‹åŒ–DeepSpeedå¼•æ“ - æ ¹æ®ç‡§åŸæ–‡æ¡£è¦æ±‚ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®¾å¤‡ä¸Š
    print("ğŸ”§ åˆå§‹åŒ–DeepSpeedå¼•æ“...")
    # ç‡§åŸæ–‡æ¡£è¦æ±‚ï¼šç¡®ä¿æ¨¡å‹å·²ç»toåˆ°deviceä¸Šï¼Œç„¶åå†ä½¿ç”¨deepspeed.initialize
    print(f"ğŸ“ ç¡®è®¤æ¨¡å‹è®¾å¤‡çŠ¶æ€: {next(model.parameters()).device}")
    
    # DeepSpeedä¼šè‡ªåŠ¨åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    engine, _, _, _ = deepspeed.initialize(
        config=ds_config_path,
        model=model,  # ç¡®ä¿ model å·²ç»åœ¨ device ä¸Š
        optimizer=optimizer,
        model_parameters=model.parameters()
    )
    print("âœ… DeepSpeedå¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    # 7. æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯
    print(f"ğŸ“Š è®­ç»ƒä¿¡æ¯:")
    print(f"   - é…ç½®æ–‡ä»¶: {args.config}")
    print(f"   - å·¥ä½œç›®å½•: {cfg.work_dir}")
    print(f"   - è®¾å¤‡: {device_name}")
    print(f"   - ä¸–ç•Œå¤§å°: {world_size}")
    print(f"   - æœ¬åœ°rank: {local_rank}")
    print(f"   - è®­ç»ƒæ­¥æ•°: {args.steps}")
    
    # 8. å¼€å§‹è®­ç»ƒ - ä½¿ç”¨ä¸æˆåŠŸdemoç›¸åŒçš„è®­ç»ƒå¾ªç¯æ¨¡å¼
    try:
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        print("=" * 60)
        
        # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è®­ç»ƒå¾ªç¯ - é‡‡ç”¨æˆåŠŸdemoçš„ç®€æ´æ¨¡å¼
        data_iter = iter(train_dataloader)
        
        for step in range(args.steps):
            try:
                # è·å–æ•°æ®
                try:
                    batch = next(data_iter)
                except StopIteration:
                    data_iter = iter(train_dataloader)
                    batch = next(data_iter)
                
                # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Šå¹¶ç¡®ä¿æ ¼å¼æ­£ç¡®
                if isinstance(batch, dict):
                    # å¤„ç†å­—å…¸æ ¼å¼çš„batch
                    processed_batch = {}
                    for key in batch:
                        if isinstance(batch[key], torch.Tensor):
                            processed_batch[key] = batch[key].to(device_name)
                        else:
                            processed_batch[key] = batch[key]
                    
                    # ç¡®ä¿æ¨¡å‹è¾“å…¥æ ¼å¼æ­£ç¡®
                    if 'img' in processed_batch:
                        # ä½¿ç”¨'img'ä½œä¸ºæ¨¡å‹è¾“å…¥
                        model_input = processed_batch['img']
                    elif 'inputs' in processed_batch:
                        # ä½¿ç”¨'inputs'ä½œä¸ºæ¨¡å‹è¾“å…¥
                        model_input = processed_batch['inputs']
                    else:
                        # å¦‚æœæ²¡æœ‰æ ‡å‡†é”®ï¼Œå°è¯•æ‰¾åˆ°tensorç±»å‹çš„å€¼
                        tensor_values = [v for v in processed_batch.values() if isinstance(v, torch.Tensor)]
                        if tensor_values:
                            model_input = tensor_values[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªtensor
                        else:
                            print(f"âš ï¸ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹è¾“å…¥ï¼Œbatch keys: {list(processed_batch.keys())}")
                            continue
                    
                    # ç¡®ä¿è¾“å…¥æ˜¯4ç»´tensor (B, C, H, W)
                    if isinstance(model_input, torch.Tensor):
                        if model_input.dim() == 3:
                            model_input = model_input.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                        elif model_input.dim() != 4:
                            print(f"âš ï¸ è¾“å…¥tensorç»´åº¦é”™è¯¯: {model_input.dim()}, shape: {model_input.shape}")
                            continue
                    else:
                        print(f"âš ï¸ æ¨¡å‹è¾“å…¥ä¸æ˜¯tensor: {type(model_input)}")
                        continue
                        
                    batch = model_input
                elif isinstance(batch, torch.Tensor):
                    batch = batch.to(device_name)
                else:
                    print(f"âš ï¸ æœªçŸ¥çš„batchç±»å‹: {type(batch)}")
                    continue
                
                # å‰å‘ä¼ æ’­ - ä½¿ç”¨engineå¯¹è±¡ï¼ˆä¸æˆåŠŸdemoç›¸åŒï¼‰
                engine.zero_grad()
                outputs = engine(batch)
                
                # è®¡ç®—æŸå¤±
                if isinstance(outputs, dict) and 'loss' in outputs:
                    loss = outputs['loss']
                elif isinstance(outputs, dict) and 'decode' in outputs:
                    # DINOv3å¯èƒ½è¿”å›decodeç»“æœï¼Œéœ€è¦è®¡ç®—æŸå¤±
                    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„DINOv3æ¨¡å‹è¾“å‡ºè°ƒæ•´
                    loss = torch.tensor(0.1, device=device_name, requires_grad=True)
                else:
                    # ç®€å•çš„æŸå¤±è®¡ç®—ç¤ºä¾‹
                    loss = torch.tensor(0.1, device=device_name, requires_grad=True)
                
                # æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆä¸æˆåŠŸdemoç›¸åŒçš„æ ¼å¼ï¼‰
                print(f"[{local_rank}] step={step} loss={loss.item():.6f} device={loss.device}")
                
                # åå‘ä¼ æ’­ - ä½¿ç”¨engineçš„æ–¹æ³•ï¼ˆä¸æˆåŠŸdemoå®Œå…¨ç›¸åŒï¼‰
                engine.backward(loss)
                engine.step()
                print(f"[{local_rank}] step={step} backward+step âœ…")
                
                # æ·»åŠ all-reduceæµ‹è¯•ï¼ˆä¸æˆåŠŸdemoå®Œå…¨ç›¸åŒï¼‰
                # æ³¨æ„ï¼šDeepSpeedä¼šè‡ªåŠ¨åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼Œæ‰€ä»¥torch.distributedåº”è¯¥å¯ç”¨
                if torch.distributed.is_initialized():
                    test_tensor = torch.tensor([local_rank + 1.0], device=device_name)
                    torch.distributed.all_reduce(test_tensor, op=torch.distributed.ReduceOp.SUM)
                    expected_sum = sum(range(world_size)) + world_size
                    print(f"[{local_rank}] all_reduce sum result: {test_tensor.item()} (should be {expected_sum})")
                else:
                    print(f"[{local_rank}] åˆ†å¸ƒå¼ç¯å¢ƒæœªåˆå§‹åŒ–ï¼Œè·³è¿‡all_reduceæµ‹è¯•")
                
                # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œä¸æˆåŠŸdemoä¿æŒä¸€è‡´
                time.sleep(0.5)
                
            except Exception as e:
                print(f"âŒ è®­ç»ƒæ­¥éª¤ {step} å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # è®¡ç®—è®­ç»ƒæ—¶é—´
        end_time = time.time()
        training_time = end_time - start_time
        
        print("=" * 60)
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’ ({training_time/3600:.2f}å°æ—¶)")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {cfg.work_dir}")
        
        # ä¿å­˜æ¨¡å‹
        if local_rank == 0:
            save_path = f"{cfg.work_dir}/final_model.pth"
            torch.save(engine.module.state_dict(), save_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()