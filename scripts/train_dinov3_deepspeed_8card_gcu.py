#!/usr/bin/env python3
"""
ç‡§åŸT20 DeepSpeedè®­ç»ƒè„šæœ¬ (ç”Ÿäº§ç‰ˆ)
ä½¿ç”¨MMEngineæ„å»ºç»„ä»¶ï¼ŒDeepSpeedé©±åŠ¨è®­ç»ƒ
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Optional, Any

# é¡¹ç›®è·¯å¾„é…ç½®
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
os.environ.setdefault('PYTORCH_GCU_ALLOC_CONF', 'backend:topsMallocAsync')

import torch
import numpy as np
from torch.utils.data.dataloader import default_collate

# æ¡ä»¶å¯¼å…¥æ¨¡å—ï¼Œé¿å…åœ¨å¼€å‘ç¯å¢ƒä¸­çš„å¯¼å…¥é”™è¯¯
torch_gcu_available = False
deepspeed_available = False
mmengine_available = False

# ç±»å‹æ³¨è§£å˜é‡
Config: Optional[Any] = None
MODELS: Optional[Any] = None
DATASETS: Optional[Any] = None
collate: Optional[Any] = None
torch_gcu: Optional[Any] = None
deepspeed: Optional[Any] = None

try:
    import torch_gcu  # type: ignore
    torch_gcu_available = True
except ImportError:
    torch_gcu = None

try:
    import deepspeed  # type: ignore
    deepspeed_available = True
except ImportError:
    deepspeed = None

try:
    from mmengine.config import Config  # type: ignore
    from mmengine.registry import MODELS, DATASETS  # type: ignore
    from mmengine.dataset import pseudo_collate as collate  # type: ignore
    mmengine_available = True
except ImportError:
    Config = None
    MODELS = None
    DATASETS = None
    collate = None

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ï¼ˆä»…åœ¨MMEngineå¯ç”¨æ—¶ï¼‰
if mmengine_available:
    try:
        import mmseg_custom.models  # type: ignore
        import mmseg_custom.datasets  # type: ignore
        import mmseg_custom.transforms  # type: ignore
    except ImportError:
        pass


def mmseg_collate_fn(batch, pad_value=0):
    """
    mmsegmentation-style collate_fn:
    - è‡ªåŠ¨æŠŠ numpy è½¬ torch.Tensor
    - è‡ªåŠ¨ pad ä¿è¯ batch å†…å›¾åƒå°ºå¯¸ä¸€è‡´
    - ä¿æŒ dict ç»“æ„ (inputs / gt_semantic_seg)
    """
    # batch: list[dict]
    elem = batch[0]
    if isinstance(elem, dict):
        collated = {}
        for key in elem:
            values = [d[key] for d in batch]

            # === è‡ªåŠ¨æŠŠ numpy è½¬ tensor ===
            if isinstance(values[0], np.ndarray):
                values = [torch.from_numpy(v) for v in values]
            elif isinstance(values[0], list) and len(values[0]) > 0 and isinstance(values[0][0], np.ndarray):
                # å¤„ç† list of numpy arrays
                values = [[torch.from_numpy(arr) for arr in v] for v in values]

            # === å›¾åƒæ•°æ® (inputs) ===
            if key == "inputs":
                # å¦‚æœæ˜¯ list of tensorsï¼Œå…ˆå¤„ç†æˆç»Ÿä¸€æ ¼å¼
                if isinstance(values[0], list):
                    # å±•å¹³æˆå•ä¸ª tensor åˆ—è¡¨
                    flat_tensors = []
                    for v in values:
                        flat_tensors.extend(v)
                    values = flat_tensors
                
                # æ‰¾å‡ºæœ€å¤§é«˜å®½
                max_h = max(v.shape[-2] for v in values if hasattr(v, 'shape'))
                max_w = max(v.shape[-1] for v in values if hasattr(v, 'shape'))
                padded = []
                for v in values:
                    if hasattr(v, 'dim') and v.dim() == 3:  # [C, H, W]
                        c, h, w = v.shape
                        pad = torch.full((c, max_h, max_w), pad_value, dtype=v.dtype)
                        pad[:, :h, :w] = v
                        padded.append(pad)
                    elif hasattr(v, 'dim') and v.dim() == 4:  # [B, C, H, W] - å·²ç»æ˜¯æ‰¹æ¬¡
                        padded.append(v)
                
                collated[key] = torch.stack(padded, dim=0)  # [B,C,H,W]

            # === æ ‡ç­¾æ•°æ® (gt_semantic_seg) ===
            elif key == "gt_semantic_seg":
                # å¦‚æœæ˜¯ list of tensorsï¼Œå…ˆå¤„ç†æˆç»Ÿä¸€æ ¼å¼
                if isinstance(values[0], list):
                    # å±•å¹³æˆå•ä¸ª tensor åˆ—è¡¨
                    flat_tensors = []
                    for v in values:
                        flat_tensors.extend(v)
                    values = flat_tensors
                
                max_h = max(v.shape[-2] for v in values if hasattr(v, 'shape'))
                max_w = max(v.shape[-1] for v in values if hasattr(v, 'shape'))
                padded = []
                for v in values:
                    if hasattr(v, 'dim') and v.dim() == 2:  # [H, W]
                        h, w = v.shape
                        pad = torch.full((max_h, max_w), pad_value, dtype=torch.long)
                        pad[:h, :w] = v.long()
                        padded.append(pad)
                    elif hasattr(v, 'dim') and v.dim() == 3:  # [1, H, W] or [C, H, W]
                        if v.shape[0] == 1:  # [1, H, W]
                            h, w = v.shape[-2:]
                            pad = torch.full((1, max_h, max_w), pad_value, dtype=torch.long)
                            pad[:, :h, :w] = v.long()
                            padded.append(pad)
                        else:  # [C, H, W] - å–ç¬¬ä¸€ä¸ªé€šé“ä½œä¸ºæ ‡ç­¾
                            h, w = v.shape[-2:]
                            pad = torch.full((max_h, max_w), pad_value, dtype=torch.long)
                            pad[:h, :w] = v[0].long()  # å–ç¬¬ä¸€ä¸ªé€šé“
                            padded.append(pad)
                
                collated[key] = torch.stack(padded, dim=0)  # [B,H,W] or [B,1,H,W]

            else:
                # å…¶ä»– key ç”¨é»˜è®¤æ–¹å¼
                try:
                    collated[key] = default_collate(values)
                except:
                    # å¦‚æœé»˜è®¤ collate å¤±è´¥ï¼Œä¿æŒåŸæ ·
                    collated[key] = values

        return collated

    else:
        return default_collate(batch)


def build_components(cfg: Any, device_name: str) -> tuple:
    """æ„å»ºè®­ç»ƒç»„ä»¶"""
    if not mmengine_available or DATASETS is None or MODELS is None:
        raise RuntimeError("MMEngine not available")
    
    # æ„å»ºæ•°æ®é›†
    dataset = DATASETS.build(cfg.train_dataloader.dataset)
    
    # æ„å»ºæ¨¡å‹
    model = MODELS.build(cfg.model)
    
    # è®¾ç½®è®¾å¤‡ - ç›´æ¥ä½¿ç”¨device_nameå­—ç¬¦ä¸²ï¼Œå…¼å®¹MMEngineçš„.to()æ–¹æ³•
    model = model.to(device_name)
    
    return model, dataset


def main() -> None:
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # ğŸ”§ å¼ºåŒ–ç‰ˆGCUå…¼å®¹æ€§è®¾ç½® - å½»åº•ç¦ç”¨æ‰€æœ‰CUDAç‰¹å®šåŠŸèƒ½
    # è¿™æ˜¯è§£å†³IndexError: list index out of rangeçš„å…³é”®ç¯å¢ƒå˜é‡è®¾ç½®
    
    # DeepSpeed CUDAç‰¹å®šç»„ä»¶ç¦ç”¨
    os.environ['DEEPSPEED_DISABLE_FUSED_ADAM'] = '1'
    os.environ['DS_BUILD_FUSED_ADAM'] = '0'
    os.environ['DS_BUILD_CPU_ADAM'] = '1'  # å¼ºåˆ¶ä½¿ç”¨CPUç‰ˆæœ¬çš„Adam
    os.environ['DS_BUILD_UTILS'] = '0'  # ç¦ç”¨å…¶ä»–CUDAç‰¹å®šå·¥å…·
    os.environ['DS_BUILD_AIO'] = '0'  # ç¦ç”¨å¼‚æ­¥IOï¼ˆå¯èƒ½ä¾èµ–CUDAï¼‰
    os.environ['DS_BUILD_SPARSE_ATTN'] = '0'  # ç¦ç”¨ç¨€ç–æ³¨æ„åŠ›ï¼ˆCUDAç‰¹å®šï¼‰
    
    # é¢å¤–çš„CUDAç‰¹å®šåŠŸèƒ½ç¦ç”¨
    os.environ['DS_BUILD_FUSED_LAMB'] = '0'  # ç¦ç”¨FusedLambä¼˜åŒ–å™¨
    os.environ['DS_BUILD_TRANSFORMER'] = '0'  # ç¦ç”¨CUDA Transformerå†…æ ¸
    os.environ['DS_BUILD_STOCHASTIC_TRANSFORMER'] = '0'  # ç¦ç”¨éšæœºTransformer
    os.environ['DS_BUILD_TRANSFORMER_INFERENCE'] = '0'  # ç¦ç”¨Transformeræ¨ç†å†…æ ¸
    os.environ['DS_BUILD_QUANTIZER'] = '0'  # ç¦ç”¨é‡åŒ–å™¨ï¼ˆå¯èƒ½ä¾èµ–CUDAï¼‰
    os.environ['DS_BUILD_RANDOM_LTD'] = '0'  # ç¦ç”¨éšæœºLTD
    
    # PyTorch CUDAç›¸å…³è®¾ç½®
    os.environ['CUDA_VISIBLE_DEVICES'] = ''  # éšè—CUDAè®¾å¤‡
    os.environ['TORCH_CUDA_ARCH_LIST'] = ''  # æ¸…ç©ºCUDAæ¶æ„åˆ—è¡¨
    
    # å¼ºåˆ¶ä½¿ç”¨CPUåç«¯è¿›è¡ŒæŸäº›æ“ä½œ
    os.environ['OMP_NUM_THREADS'] = '4'  # é™åˆ¶OpenMPçº¿ç¨‹æ•°
    
    parser = argparse.ArgumentParser(description='DeepSpeed Training')
    parser.add_argument('--config', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--work-dir', required=True, help='å·¥ä½œç›®å½•')
    parser.add_argument('--deepspeed', required=True, help='DeepSpeedé…ç½®æ–‡ä»¶')
    parser.add_argument('--launcher', default='deepspeed', help='å¯åŠ¨å™¨ç±»å‹')
    parser.add_argument('--local_rank', type=int, default=0, help='æœ¬åœ°rank')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å¿…è¦æ¨¡å—
    if not mmengine_available or Config is None:
        print("Error: MMEngine not available")
        return
    
    if not deepspeed_available or deepspeed is None:
        print("Error: DeepSpeed not available")
        return
    
    print(f"ğŸ”§ å·²è®¾ç½®ç¯å¢ƒå˜é‡ç¦ç”¨FusedAdamå’Œå…¶ä»–CUDAç‰¹å®šç»„ä»¶ï¼Œç¡®ä¿GCUå…¼å®¹æ€§")
    
    # åŠ è½½é…ç½®
    cfg = Config.fromfile(args.config)
    
    # ç¯å¢ƒè®¾ç½® - ä½¿ç”¨xlaè®¾å¤‡æ ¼å¼ä»¥å…¼å®¹MMEngine
    if torch_gcu_available and torch_gcu is not None:
        device_id = torch_gcu.current_device()
        device_name = f'xla:{device_id}'
    else:
        device_name = 'cuda'
    
    # æ„å»ºç»„ä»¶
    model, dataset = build_components(cfg, device_name)
    
    # åŠ è½½DeepSpeedé…ç½®
    with open(args.deepspeed, 'r') as f:
        deepspeed_config = json.load(f)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=deepspeed_config.get('train_micro_batch_size_per_gpu', 8),
        shuffle=True,
        collate_fn=collate if collate else None,  # ğŸ”§ æš‚æ—¶ä½¿ç”¨åŸæ¥çš„ collateï¼Œåœ¨è®­ç»ƒå¾ªç¯ä¸­æ‰‹åŠ¨å¤„ç†
        num_workers=4
    )
    
    # ğŸ”§ åˆå§‹åŒ–DeepSpeed - æ‰‹åŠ¨åˆ›å»ºä¼˜åŒ–å™¨é¿å…FusedAdamç¼–è¯‘é—®é¢˜
    # è¿™æ˜¯è§£å†³ IndexError: list index out of range çš„æœ€ç»ˆæ–¹æ¡ˆ
    # é€šè¿‡æ‰‹åŠ¨åˆ›å»ºæ ‡å‡†PyTorchä¼˜åŒ–å™¨ï¼Œç»•è¿‡DeepSpeedå†…éƒ¨çš„CUDAç‰¹å®šä»£ç è·¯å¾„
    print("ğŸ”§ æ­£åœ¨æ‰‹åŠ¨åˆ›å»ºä¼˜åŒ–å™¨ï¼Œç¡®ä¿GCUå…¼å®¹æ€§...")
    
    # ğŸ”§ å…³é”®ä¿®æ­£ (1/2): ä»é…ç½®ä¸­è·å–ä¼˜åŒ–å™¨å‚æ•°å¹¶æ‰‹åŠ¨åˆ›å»º
    optimizer_params = deepspeed_config.get('optimizer', {}).get('params', {})
    optimizer = torch.optim.AdamW(model.parameters(), **optimizer_params)
    print(f"âœ… æ‰‹åŠ¨åˆ›å»ºä¼˜åŒ–å™¨æˆåŠŸ: {type(optimizer).__name__}")
    
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–DeepSpeedï¼Œä½¿ç”¨æ‰‹åŠ¨åˆ›å»ºçš„ä¼˜åŒ–å™¨...")
    
    # ğŸ”§ å…³é”®ä¿®æ­£ (2/2): å°†æ‰‹åŠ¨åˆ›å»ºçš„optimizerå®ä¾‹ä¼ é€’ç»™initializeå‡½æ•°
    # è¿™é¿å…äº†DeepSpeedå†…éƒ¨å°è¯•ç¼–è¯‘FusedAdamçš„é—®é¢˜
    model_engine, optimizer, _, _ = deepspeed.initialize(
        model=model,
        model_parameters=model.parameters(),  # å…³é”®ï¼šæä¾›æ¨¡å‹å‚æ•°ç»™DeepSpeed
        optimizer=optimizer,  # ğŸ”§ å…³é”®ï¼šä¼ å…¥æ‰‹åŠ¨åˆ›å»ºçš„ä¼˜åŒ–å™¨
        config=deepspeed_config
    )
    
    print("DeepSpeedè®­ç»ƒå¼€å§‹...")
    
    # ğŸ”¥ ä¿®å¤çš„è®­ç»ƒå¾ªç¯ - æ­£ç¡®å¤„ç†æ‰¹æ¬¡æ•°æ®æ ¼å¼
    for step, batch in enumerate(dataloader):
        if step >= 10:  # é™åˆ¶æ­¥æ•°ç”¨äºæµ‹è¯•
            break
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ­£ç¡®æå–å’Œå¤„ç†æ‰¹æ¬¡æ•°æ®
        if step == 0:
            print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - Batch ç»“æ„: {type(batch)}")
            if isinstance(batch, dict):
                print(f"ğŸ” Batch keys: {list(batch.keys())}")
                if 'inputs' in batch:
                    print(f"ğŸ” inputs å½¢çŠ¶: {batch['inputs'].shape if hasattr(batch['inputs'], 'shape') else type(batch['inputs'])}")
                if 'data_samples' in batch:
                    print(f"ğŸ” data_samples ç±»å‹: {type(batch['data_samples'])}")
        
        # æ ¹æ® MMEngine æ ‡å‡†ï¼Œæ¨¡å‹æœŸæœ›æ¥æ”¶ inputs å’Œ data_samples
        if isinstance(batch, dict) and 'inputs' in batch:
            # æ ‡å‡† MMEngine æ ¼å¼
            inputs = batch['inputs']
            data_samples = batch.get('data_samples', None)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿ inputs æ˜¯æ­£ç¡®çš„ 4D tensor (B, C, H, W)
        if isinstance(inputs, list):
            print(f"[DEBUG] inputs is list, stacking {len(inputs)} tensors...")
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸åŒå°ºå¯¸çš„å›¾åƒ
            shapes = [inp.shape for inp in inputs]
            print(f"[DEBUG] input shapes: {shapes}")
            
            # å¦‚æœå°ºå¯¸ä¸ä¸€è‡´ï¼Œéœ€è¦ resize åˆ°ç»Ÿä¸€å¤§å°
            if len(set(shapes)) > 1:
                print("[DEBUG] Found different input sizes, resizing to common size...")
                # æ‰¾åˆ°æœ€å¤§å°ºå¯¸
                max_h = max(inp.shape[-2] for inp in inputs)
                max_w = max(inp.shape[-1] for inp in inputs)
                print(f"[DEBUG] Target size: {max_h}x{max_w}")
                
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ F.pad ç¡®ä¿å†…å­˜è¿ç»­æ€§å’Œæ­£ç¡®çš„å°ºå¯¸è®¡ç®—
                import torch.nn.functional as F
                padded_inputs = []
                for inp in inputs:
                    c, h, w = inp.shape
                    if h != max_h or w != max_w:
                        # è®¡ç®—éœ€è¦çš„ padding
                        pad_h = max_h - h
                        pad_w = max_w - w
                        # F.pad æ ¼å¼: (left, right, top, bottom)
                        padded = F.pad(inp, (0, pad_w, 0, pad_h), mode='constant', value=0)
                        # ç¡®ä¿å†…å­˜è¿ç»­æ€§
                        padded = padded.contiguous()
                        padded_inputs.append(padded)
                    else:
                        # ç¡®ä¿åŸå§‹å¼ é‡ä¹Ÿæ˜¯è¿ç»­çš„
                        padded_inputs.append(inp.contiguous())
                inputs = padded_inputs
            
            inputs = torch.stack(inputs, dim=0)
            print(f"[DEBUG] after stacking: {inputs.shape}")
        elif isinstance(inputs, torch.Tensor) and inputs.dim() == 3:
            print("[DEBUG] single image tensor, unsqueezing batch dim...")
            inputs = inputs.unsqueeze(0)
            print(f"[DEBUG] after unsqueeze: {inputs.shape}")
        
        # ğŸ”§ æ··åˆç²¾åº¦ä¿®å¤ï¼šä½¿ç”¨æ¨¡å‹å‚æ•°çš„çœŸå® device å’Œ dtype
        # è·å– DeepSpeed åŒ…è£¹åçš„çœŸå®æ¨¡å‹å‚æ•°ä¿¡æ¯
        device = next(model_engine.parameters()).device
        dtype = next(model_engine.parameters()).dtype
        
        # ğŸ”§ T20 å†…å­˜å®‰å…¨ä¿®å¤ï¼šåˆ†æ­¥éª¤è¿›è¡Œè®¾å¤‡è½¬æ¢ï¼Œé¿å…å†…å­˜æŒ‡é’ˆé”™è¯¯
        print(f"[DEBUG] Converting inputs to device: {device}, dtype: {dtype}")
        print(f"[DEBUG] Original inputs device: {inputs.device}, dtype: {inputs.dtype}")
        
        # å…ˆç¡®ä¿å¼ é‡åœ¨ CPU ä¸Šä¸”å†…å­˜è¿ç»­
        if inputs.device != torch.device('cpu'):
            inputs = inputs.cpu().contiguous()
        
        # åˆ†æ­¥è½¬æ¢ï¼šå…ˆè½¬æ¢æ•°æ®ç±»å‹ï¼Œå†è½¬æ¢è®¾å¤‡
        if inputs.dtype != dtype:
            inputs = inputs.to(dtype=dtype)
            print(f"[DEBUG] Converted dtype to: {inputs.dtype}")
        
        # æœ€åè½¬æ¢è®¾å¤‡ï¼Œä½¿ç”¨ non_blocking=False ç¡®ä¿åŒæ­¥è½¬æ¢
        if inputs.device != device:
            inputs = inputs.to(device=device, non_blocking=False)
            print(f"[DEBUG] Converted device to: {inputs.device}")
        
        print(f"[DEBUG] final inputs shape: {inputs.shape}, device: {inputs.device}, dtype: {inputs.dtype}")
        
        # ğŸ”§ å¤„ç† batch ä¸­çš„ gt_semantic_segï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'gt_semantic_seg' in batch:
            gt_seg = batch['gt_semantic_seg']
            print(f"[DEBUG] gt_semantic_seg type: {type(gt_seg)}")
            
            if isinstance(gt_seg, list):
                print(f"[DEBUG] gt_semantic_seg is list, len={len(gt_seg)}")
                print(f"[DEBUG] gt_semantic_seg element types: {[type(x) for x in gt_seg]}")
                
                # å…ˆå°† numpy è½¬æ¢ä¸º tensor
                gt_tensors = []
                for i, seg in enumerate(gt_seg):
                    if isinstance(seg, np.ndarray):
                        gt_tensors.append(torch.from_numpy(seg))
                    elif isinstance(seg, torch.Tensor):
                        gt_tensors.append(seg)
                    else:
                        raise TypeError(f"Unexpected gt_semantic_seg[{i}] type: {type(seg)}")
                
                # æ£€æŸ¥å°ºå¯¸æ˜¯å¦ä¸€è‡´
                shapes = [t.shape for t in gt_tensors]
                print(f"[DEBUG] gt_semantic_seg shapes: {shapes}")
                
                if len(set(shapes)) > 1:
                    print("[DEBUG] Found different gt_semantic_seg sizes, padding to common size...")
                    # æ‰¾åˆ°æœ€å¤§å°ºå¯¸
                    max_h = max(t.shape[-2] for t in gt_tensors)
                    max_w = max(t.shape[-1] for t in gt_tensors)
                    print(f"[DEBUG] Target gt_semantic_seg size: {max_h}x{max_w}")
                    
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ F.pad ç¡®ä¿å†…å­˜è¿ç»­æ€§
                    import torch.nn.functional as F
                    padded_gts = []
                    for t in gt_tensors:
                        if t.dim() == 2:  # [H, W]
                            h, w = t.shape
                            if h != max_h or w != max_w:
                                # è®¡ç®—éœ€è¦çš„ padding
                                pad_h = max_h - h
                                pad_w = max_w - w
                                # F.pad æ ¼å¼: (left, right, top, bottom)
                                padded = F.pad(t.long(), (0, pad_w, 0, pad_h), mode='constant', value=0)
                                # ç¡®ä¿å†…å­˜è¿ç»­æ€§
                                padded = padded.contiguous()
                                padded_gts.append(padded)
                            else:
                                # ç¡®ä¿åŸå§‹å¼ é‡ä¹Ÿæ˜¯è¿ç»­çš„
                                padded_gts.append(t.long().contiguous())
                        else:
                            padded_gts.append(t.long())
                    gt_tensors = padded_gts
                
                # å †å å¹¶ç§»åŠ¨åˆ°è®¾å¤‡ - ğŸ”§ T20 å†…å­˜å®‰å…¨ä¿®å¤
                print("[DEBUG] Stacking gt_semantic_seg tensors...")
                stacked_gt = torch.stack(gt_tensors)
                print(f"[DEBUG] Stacked gt_semantic_seg shape: {stacked_gt.shape}")
                
                # åˆ†æ­¥è½¬æ¢åˆ°ç›®æ ‡è®¾å¤‡ï¼Œé¿å…å†…å­˜æŒ‡é’ˆé”™è¯¯
                if stacked_gt.device != torch.device('cpu'):
                    stacked_gt = stacked_gt.cpu().contiguous()
                
                # å…ˆè½¬æ¢æ•°æ®ç±»å‹
                if stacked_gt.dtype != torch.long:
                    stacked_gt = stacked_gt.to(dtype=torch.long)
                    print(f"[DEBUG] Converted gt_semantic_seg dtype to: {stacked_gt.dtype}")
                
                # æœ€åè½¬æ¢è®¾å¤‡
                if stacked_gt.device != device:
                    stacked_gt = stacked_gt.to(device=device, non_blocking=False)
                    print(f"[DEBUG] Converted gt_semantic_seg device to: {stacked_gt.device}")
                
                batch['gt_semantic_seg'] = stacked_gt
                print(f"[DEBUG] Final gt_semantic_seg: shape={batch['gt_semantic_seg'].shape}, device={batch['gt_semantic_seg'].device}, dtype={batch['gt_semantic_seg'].dtype}")
            else:
                # å•ä¸ªå¼ é‡æˆ– numpy æ•°ç»„ - ğŸ”§ T20 å†…å­˜å®‰å…¨ä¿®å¤
                print("[DEBUG] Processing single gt_semantic_seg...")
                if isinstance(gt_seg, np.ndarray):
                    gt_seg = torch.from_numpy(gt_seg)
                    print(f"[DEBUG] Converted numpy to tensor: {gt_seg.shape}")
                
                # åˆ†æ­¥è½¬æ¢åˆ°ç›®æ ‡è®¾å¤‡ï¼Œé¿å…å†…å­˜æŒ‡é’ˆé”™è¯¯
                if gt_seg.device != torch.device('cpu'):
                    gt_seg = gt_seg.cpu().contiguous()
                
                # å…ˆè½¬æ¢æ•°æ®ç±»å‹
                if gt_seg.dtype != torch.long:
                    gt_seg = gt_seg.to(dtype=torch.long)
                    print(f"[DEBUG] Converted single gt_semantic_seg dtype to: {gt_seg.dtype}")
                
                # æœ€åè½¬æ¢è®¾å¤‡
                if gt_seg.device != device:
                    gt_seg = gt_seg.to(device=device, non_blocking=False)
                    print(f"[DEBUG] Converted single gt_semantic_seg device to: {gt_seg.device}")
                
                batch['gt_semantic_seg'] = gt_seg
                print(f"[DEBUG] Final single gt_semantic_seg: shape={batch['gt_semantic_seg'].shape}, device={batch['gt_semantic_seg'].device}, dtype={batch['gt_semantic_seg'].dtype}")
            
            # ğŸ”§ å°†ç›‘ç£ä¿¡å·ä¹Ÿè½¬ç§»åˆ°ç›¸åŒè®¾å¤‡
            if data_samples is not None:
                for i, sample in enumerate(data_samples):
                    if hasattr(sample, 'gt_sem_seg') and sample.gt_sem_seg is not None:
                        if hasattr(sample.gt_sem_seg, 'data'):
                            sample.gt_sem_seg.data = sample.gt_sem_seg.data.to(device=device)
                            print(f"[DEBUG] gt_sem_seg[{i}] moved to device: {device}")
            
            # è°ƒç”¨æ¨¡å‹çš„ forward æ–¹æ³•
            loss_dict = model_engine(inputs, data_samples, mode='loss')
            
            # å¤„ç†è¿”å›çš„ loss
            if isinstance(loss_dict, dict):
                loss = loss_dict.get('loss', loss_dict.get('decode.loss_ce', list(loss_dict.values())[0]))
            else:
                loss = loss_dict
                
        else:
            # å…œåº•å¤„ç†ï¼šç›´æ¥ä¼ é€’æ•´ä¸ª batch
            print(f"âš ï¸ è­¦å‘Šï¼šä½¿ç”¨å…œåº•å¤„ç†ï¼Œç›´æ¥ä¼ é€’ batch")
            loss = model_engine(batch)
        
        model_engine.backward(loss)
        model_engine.step()
        
        if step % 5 == 0:
            loss_value = loss.item() if hasattr(loss, 'item') else loss
            print(f"Step {step}, Loss: {loss_value}")
    
    print("è®­ç»ƒå®Œæˆ")


if __name__ == '__main__':
    main()