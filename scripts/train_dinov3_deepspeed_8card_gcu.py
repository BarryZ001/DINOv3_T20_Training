#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DINOv3 + MMRS-1M 8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ (åŸºäºDeepSpeed)
ä½¿ç”¨DeepSpeedæ¡†æ¶è¿›è¡ŒGCUç¯å¢ƒä¸‹çš„åˆ†å¸ƒå¼è®­ç»ƒ
"""
import argparse
import os
import sys
import time
import warnings
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è®¾ç½®GCUç¯å¢ƒå˜é‡
os.environ.setdefault('PYTORCH_GCU_ALLOC_CONF', 'backend:topsMallocAsync')
os.environ.setdefault('TORCH_ECCL_AVOID_RECORD_STREAMS', 'false')
os.environ.setdefault('TORCH_ECCL_ASYNC_ERROR_HANDLING', '3')

# å¯¼å…¥å¿…è¦çš„åº“
try:
    import torch
    print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    import torch_gcu  # ç‡§åŸGCUæ”¯æŒ
    print(f"âœ… torch_gcuå¯ç”¨: {torch_gcu.is_available()}")
    if torch_gcu.is_available():
        print(f"âœ… GCUè®¾å¤‡æ•°: {torch_gcu.device_count()}")
    else:
        raise RuntimeError("torch_gcuä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…")
    
    import deepspeed
    print(f"âœ… DeepSpeedç‰ˆæœ¬: {deepspeed.__version__}")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

try:
    from mmengine.config import Config
    from mmengine.registry import MODELS, DATASETS
    print("âœ… MMEngineå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ MMEngineå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

try:
    import mmseg
    from mmseg.models import *
    from mmseg.apis import init_segmentor
    print("âœ… MMSegmentationå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ MMSegmentationå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    import mmseg_custom.models
    import mmseg_custom.datasets  # è¿™ä¼šæ³¨å†ŒMMRS1MDatasetåˆ°MMEngineçš„DATASETS
    import mmseg_custom.transforms
    print("âœ… è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
except ImportError as e:
    print(f"âš ï¸ è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    # å°è¯•æ‰‹åŠ¨å¯¼å…¥å…³é”®ç»„ä»¶
    try:
        from mmseg_custom.datasets.mmrs1m_dataset import MMRS1MDataset
        from mmseg_custom.datasets.loveda_dataset import LoveDADataset
        print("âœ… æ‰‹åŠ¨å¯¼å…¥æ•°æ®é›†ç±»æˆåŠŸ")
    except ImportError as e2:
        print(f"âŒ æ‰‹åŠ¨å¯¼å…¥æ•°æ®é›†å¤±è´¥: {e2}")
        sys.exit(1)

def setup_gcu_environment():
    """è®¾ç½®GCUç¯å¢ƒ - å·²åºŸå¼ƒï¼Œä½¿ç”¨mainå‡½æ•°ä¸­çš„ç®€åŒ–ç‰ˆæœ¬"""
    # è¿™ä¸ªå‡½æ•°å·²è¢«åºŸå¼ƒï¼Œç°åœ¨ç›´æ¥åœ¨mainå‡½æ•°ä¸­ä½¿ç”¨ä¸æˆåŠŸdemoç›¸åŒçš„æ–¹å¼
    pass

def make_deepspeed_config(config_path="/tmp/ds_config.json"):
    """åˆ›å»ºDeepSpeedé…ç½®æ–‡ä»¶ - ä¼˜åŒ–ä¸º8å¡åˆ†å¸ƒå¼è®­ç»ƒ"""
    cfg = {
        "train_batch_size": 64,  # 8å¡æ€»batch size (8 * 8 = 64)
        "train_micro_batch_size_per_gpu": 8,  # æ¯ä¸ªGPUçš„micro batch size
        "gradient_accumulation_steps": 1,
        "fp16": {"enabled": False},  # GCUç¯å¢ƒä¸‹æš‚æ—¶ä¸ä½¿ç”¨fp16
        "zero_optimization": {
            "stage": 2,  # ä½¿ç”¨ZeRO-2ä¼˜åŒ–ï¼Œé€‚åˆ8å¡è®­ç»ƒ
            "allgather_partitions": True,
            "allgather_bucket_size": 2e8,
            "overlap_comm": True,
            "reduce_scatter": True,
            "reduce_bucket_size": 2e8,
            "contiguous_gradients": True
        },
        "optimizer": {
            "type": "Adam",
            "params": {
                "lr": 1e-4,
                "betas": [0.9, 0.999],
                "eps": 1e-8,
                "weight_decay": 0.01
            }
        },
        "scheduler": {
            "type": "WarmupLR",
            "params": {
                "warmup_min_lr": 0,
                "warmup_max_lr": 1e-4,
                "warmup_num_steps": 100
            }
        },
        "steps_per_print": 10,
        "wall_clock_breakdown": False,
        "comms_logger": {"enabled": True},  # å¯ç”¨é€šä¿¡æ—¥å¿—ä»¥ç›‘æ§8å¡é€šä¿¡
        "tensorboard": {"enabled": True, "output_path": "./tensorboard_logs"},
        "flops_profiler": {"enabled": False}
    }
    
    with open(config_path, "w") as f:
        json.dump(cfg, f, indent=2)
    
    print(f"ğŸ“ DeepSpeed 8å¡åˆ†å¸ƒå¼é…ç½®æ–‡ä»¶: {config_path}")
    return config_path

def load_and_validate_config(config_path, work_dir=None):
    """åŠ è½½å’ŒéªŒè¯é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    print(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    cfg = Config.fromfile(config_path)
    
    # è®¾ç½®å·¥ä½œç›®å½•
    if work_dir is not None:
        cfg.work_dir = work_dir
    elif cfg.get('work_dir', None) is None:
        cfg.work_dir = './work_dirs/dinov3_deepspeed_8card_gcu'
    
    print(f"ğŸ“ å·¥ä½œç›®å½•: {cfg.work_dir}")
    
    # ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
    os.makedirs(cfg.work_dir, exist_ok=True)
    os.makedirs(f"{cfg.work_dir}/logs", exist_ok=True)
    
    # éªŒè¯å…³é”®é…ç½®
    if not hasattr(cfg, 'model'):
        raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘modelé…ç½®")
    
    if not hasattr(cfg, 'train_dataloader'):
        raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘train_dataloaderé…ç½®")
    
    print("âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")
    return cfg

# åˆ é™¤custom_collate_fn - è®©MMEngineé…ç½®æ–‡ä»¶å¤„ç†æ•°æ®æ ¼å¼é—®é¢˜

def build_model_and_dataset(cfg, device_name):
    """æ„å»ºæ¨¡å‹å’Œæ•°æ®é›†"""
    print(f"ğŸ“Š æ„å»ºæ•°æ®é›†: {cfg.train_dataloader.dataset.type}")
    
    # ä½¿ç”¨MMEngineçš„ç»Ÿä¸€æ„å»ºå™¨æ„å»ºè®­ç»ƒæ•°æ®é›†
    train_dataset = DATASETS.build(cfg.train_dataloader.dataset)
    print(f"âœ… è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
    
    # æ„å»ºéªŒè¯æ•°æ®é›†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    val_dataset = None
    if hasattr(cfg, 'val_dataloader') and cfg.val_dataloader is not None:
        val_dataset = DATASETS.build(cfg.val_dataloader.dataset)
        print(f"âœ… éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset)}")
    
    # æ„å»ºæ¨¡å‹
    print(f"ğŸ—ï¸ æ„å»ºæ¨¡å‹: {cfg.model.type}")
    model = MODELS.build(cfg.model)
    print(f"âœ… æ¨¡å‹æ„å»ºå®Œæˆ")
    
    # è®¾ç½®è®¾å¤‡
    if device_name.startswith('xla'):
        device = torch_gcu.device(device_name)
        # å¯¹äºGCUè®¾å¤‡ï¼Œç›´æ¥ä½¿ç”¨device_nameå­—ç¬¦ä¸²
        model = model.to(device_name)
    else:
        device = torch.device(device_name)
        model = model.to(device)
    
    print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device_name}")
    
    return model, train_dataset, val_dataset

def setup_signal_handlers():
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼Œé˜²æ­¢æ­»å¾ªç¯"""
    import signal
    
    def signal_handler(signum, frame):
        print(f"\nâš ï¸ æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
        import sys
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()
    
    # åˆ›å»ºå‚æ•°è§£æå™¨ï¼Œæ­£ç¡®å¤„ç†DeepSpeedçš„--local_rankå‚æ•°
    parser = argparse.ArgumentParser(description='DINOv3 + MMRS-1M 8å¡åˆ†å¸ƒå¼è®­ç»ƒ')
    parser.add_argument('--config', type=str, 
                       default="configs/train_dinov3_mmrs1m_t20_gcu_8card.py",
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--work-dir', type=str, default=None,
                       help='å·¥ä½œç›®å½•')
    parser.add_argument('--local_rank', type=int, default=-1,
                       help='DeepSpeedè‡ªåŠ¨æ·»åŠ çš„local rankå‚æ•°')
    parser.add_argument('--steps', type=int, default=1000,
                       help='è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    print(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {args.work_dir}")
    print(f"ğŸ“ Local Rank: {args.local_rank}")
    print(f"ğŸ“ è®­ç»ƒæ­¥æ•°: {args.steps}")

    print("ğŸš€ å¯åŠ¨DINOv3 + MMRS-1M 8å¡åˆ†å¸ƒå¼è®­ç»ƒ")
    print("=" * 60)
    
    # 1. è®¾ç½®GCUç¯å¢ƒå’Œåˆ†å¸ƒå¼è®­ç»ƒ - ä½¿ç”¨ä¸æˆåŠŸdemoç›¸åŒçš„æ–¹å¼
    local_rank = args.local_rank if args.local_rank >= 0 else int(os.environ.get("LOCAL_RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", "8"))  # é»˜è®¤8å¡
    
    # è®¾ç½®è®¾å¤‡
    device_name = f"xla:{local_rank}"
    print(f"[PID {os.getpid()}] GCUç¯å¢ƒ - local_rank={local_rank}, world_size={world_size}, device={device_name}")
    
    # è®¾ç½®GCUè®¾å¤‡
    if torch_gcu is not None:
        torch_gcu.set_device(local_rank)
        print(f"âœ… è®¾ç½®GCUè®¾å¤‡: {local_rank}")
    else:
        print("âš ï¸ torch_gcuä¸å¯ç”¨ï¼Œè·³è¿‡è®¾å¤‡è®¾ç½®")
    
    # åˆ é™¤æ‰‹åŠ¨åˆ†å¸ƒå¼åˆå§‹åŒ– - è®©DeepSpeedå®Œå…¨æ¥ç®¡åˆ†å¸ƒå¼ç¯å¢ƒ
    print(f"ğŸ“± è®­ç»ƒæ¨¡å¼ - world_size={world_size}, local_rank={local_rank}")
    
    # 2. åŠ è½½é…ç½®
    cfg = load_and_validate_config(args.config, args.work_dir)
    
    # æ„å»ºæ¨¡å‹å’Œæ•°æ®é›†
    model, train_dataset, val_dataset = build_model_and_dataset(cfg, device_name)
    
    # ==================== æ•°æ®åŠ è½½è°ƒè¯•ä»£ç  ====================
    print("\nğŸ” === å¼€å§‹æ•°æ®åŠ è½½è°ƒè¯• ===")
    # Import the new pseudo_collate function from mmengine.dataset
    # and alias it as collate for convenience.
    from mmengine.dataset import pseudo_collate as collate
    from torch.utils.data import DataLoader
    
    # ä½¿ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„å‚æ•°æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ª DataLoader
    debug_dataloader = DataLoader(
        train_dataset,
        batch_size=2,  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ batch size
        shuffle=True,
        num_workers=2,  # ä½¿ç”¨å°‘é‡ worker ä»¥ä¾¿è°ƒè¯•
        collate_fn=collate
    )
    
    # è¿­ä»£å‡ ä¸ªæ‰¹æ¬¡å¹¶æ£€æŸ¥å†…å®¹
    for i, batch in enumerate(debug_dataloader):
        print(f"\n--- æ­£åœ¨æ£€æŸ¥ Batch #{i} ---")
        print(f"Batch keys: {list(batch.keys())}")
        
        if 'data_samples' not in batch:
            print("âŒ é”™è¯¯: æ‰¹å¤„ç†æ•°æ®ä¸­æ²¡æœ‰ 'data_samples' é”®ï¼")
            print(f"å®é™…çš„é”®: {list(batch.keys())}")
            continue
        
        print(f"Batch size: {len(batch['data_samples'])}")
        has_labels_count = 0
        
        # é¦–å…ˆæ£€æŸ¥ data_samples çš„ç±»å‹å’Œç»“æ„
        data_samples = batch['data_samples']
        print(f"  data_samples ç±»å‹: {type(data_samples)}")
        print(f"  data_samples å†…å®¹: {data_samples}")
        
        # å¦‚æœ data_samples æ˜¯å­—ç¬¦ä¸²æˆ–å…¶ä»–éå¯è¿­ä»£ç±»å‹ï¼Œè·³è¿‡å¤„ç†
        if isinstance(data_samples, str):
            print(f"  âŒ data_samples æ˜¯å­—ç¬¦ä¸²ï¼Œæ— æ³•è¿­ä»£: {data_samples}")
            continue
        elif not hasattr(data_samples, '__iter__'):
            print(f"  âŒ data_samples ä¸å¯è¿­ä»£: {type(data_samples)}")
            continue
        
        # å°è¯•è·å–é•¿åº¦
        try:
            samples_length = len(data_samples)
            print(f"  data_samples é•¿åº¦: {samples_length}")
        except:
            print(f"  âŒ æ— æ³•è·å– data_samples é•¿åº¦")
            continue
        
        # å¦‚æœæ˜¯ç©ºçš„ï¼Œè·³è¿‡
        if samples_length == 0:
            print(f"  âš ï¸ data_samples ä¸ºç©º")
            continue
        
        # å°è¯•è¿­ä»£å¤„ç†æ ·æœ¬
        try:
            for j, sample in enumerate(data_samples):
                print(f"  æ ·æœ¬ #{j}:")
                print(f"    ç±»å‹: {type(sample)}")
                print(f"    å±æ€§: {dir(sample) if hasattr(sample, '__dict__') else 'N/A'}")
                
                if hasattr(sample, 'gt_sem_seg'):
                    if sample.gt_sem_seg is not None:
                        has_labels_count += 1
                        if hasattr(sample.gt_sem_seg, 'data'):
                            print(f"    âœ… åŒ…å«æ ‡ç­¾, å½¢çŠ¶: {sample.gt_sem_seg.data.shape}")
                        else:
                            print(f"    âœ… åŒ…å«æ ‡ç­¾, ç±»å‹: {type(sample.gt_sem_seg)}")
                    else:
                        print(f"    âš ï¸ gt_sem_seg ä¸º None")
                else:
                    print(f"    âŒ ç¼ºå°‘ gt_sem_seg å±æ€§")
                    
                # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„æ ‡ç­¾å­—æ®µ
                if hasattr(sample, 'gt_semantic_seg'):
                    print(f"    å‘ç° gt_semantic_seg: {type(sample.gt_semantic_seg)}")
                
                # æ‰“å°æ ·æœ¬çš„æ‰€æœ‰å±æ€§
                if hasattr(sample, '__dict__'):
                    print(f"    æ‰€æœ‰å±æ€§: {list(sample.__dict__.keys())}")
        except Exception as e:
            print(f"  âŒ è¿­ä»£ data_samples æ—¶å‡ºé”™: {e}")
            continue
        
        print(f"æ‰¹æ¬¡ #{i} ä¸­æœ‰æ•ˆæ ‡ç­¾æ•°é‡: {has_labels_count}/{samples_length}")
        
        if has_labels_count == 0:
            print(f"âŒâŒâŒ è‡´å‘½é”™è¯¯: Batch #{i} ä¸­æ‰€æœ‰æ ·æœ¬éƒ½ç¼ºå°‘æœ‰æ•ˆæ ‡ç­¾ï¼è¿™å°±æ˜¯å¯¼è‡´ torch.stack å¤±è´¥çš„åŸå› ã€‚")
            print("è®©æˆ‘ä»¬æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯:")
            try:
                if samples_length > 0:
                    sample = data_samples[0]
                    print(f"æ ·æœ¬è¯¦ç»†ä¿¡æ¯: {sample}")
            except Exception as e:
                print(f"âŒ è®¿é—®ç¬¬ä¸€ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
        
        if i >= 3:  # åªæ£€æŸ¥å‰å‡ ä¸ªæ‰¹æ¬¡
            break
    
    print("ğŸ” === æ•°æ®åŠ è½½è°ƒè¯•ç»“æŸ ===\n")
    # ========================================================
    
    # 4. åˆ›å»ºDeepSpeedé…ç½®
    ds_config_path = make_deepspeed_config()

    # 5. åˆå§‹åŒ–DeepSpeedå¼•æ“ (æ ¸å¿ƒå˜åŒ–)
    #    - ä¸å†æ‰‹åŠ¨åˆ›å»º DataLoader 
    #    - ä¸å†æ‰‹åŠ¨åˆå§‹åŒ– torch.distributed 
    #    - å°† train_dataset ç›´æ¥äº¤ç»™ DeepSpeed 
    print("ğŸ”§ åˆå§‹åŒ–DeepSpeedå¼•æ“...")
    # Import the new pseudo_collate function from mmengine.dataset
    # and alias it as collate for convenience.
    from mmengine.dataset import pseudo_collate as collate
    
    # åˆ›å»ºä¼˜åŒ–å™¨ - ä½¿ç”¨ä¸æˆåŠŸdemoç›¸åŒçš„Adamä¼˜åŒ–å™¨
    print("ğŸ”§ åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    print("âœ… ä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆ")
    
    # ç‡§åŸæ–‡æ¡£è¦æ±‚ï¼šç¡®ä¿æ¨¡å‹å·²ç»toåˆ°deviceä¸Šï¼Œç„¶åå†ä½¿ç”¨deepspeed.initialize
    print(f"ğŸ“ ç¡®è®¤æ¨¡å‹è®¾å¤‡çŠ¶æ€: {next(model.parameters()).device}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰MPIç¯å¢ƒï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å•GPUæ¨¡å¼
    try:
        print("ğŸ”§ å°è¯•åˆå§‹åŒ–DeepSpeedå¼•æ“...")
        # å°è¯•åˆå§‹åŒ–DeepSpeed
        engine, optimizer, train_dataloader, _ = deepspeed.initialize(
            config=ds_config_path,
            model=model,  # ç¡®ä¿ model å·²ç»åœ¨ device ä¸Š
            optimizer=optimizer,
            model_parameters=model.parameters(),
            training_data=train_dataset,
            collate_fn=collate  # ä½¿ç”¨mmcvçš„collateå‡½æ•°å¤„ç†DataContainer
        )
        print("âœ… DeepSpeedå¼•æ“åŠDataLoaderåˆå§‹åŒ–å®Œæˆ")
        use_deepspeed = True
    except Exception as e:
        print(f"âš ï¸ DeepSpeedåˆå§‹åŒ–å¤±è´¥: {e}")
        print(f"âš ï¸ é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"âš ï¸ é”™è¯¯è¯¦æƒ…: {str(e)}")
        print("ğŸ”„ å›é€€åˆ°å•GPUè®­ç»ƒæ¨¡å¼...")
        
        # æ¸…ç†å¯èƒ½çš„DeepSpeedçŠ¶æ€
        try:
            import deepspeed
            deepspeed.init_distributed()
        except:
            pass
            
        # å›é€€åˆ°æ‰‹åŠ¨åˆ›å»ºDataLoader
        from torch.utils.data import DataLoader
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=cfg.train_dataloader.get('batch_size', 2),
            shuffle=True,
            num_workers=cfg.train_dataloader.get('num_workers', 2),
            pin_memory=False,  # GCUç¯å¢ƒä¸‹ä¸ä½¿ç”¨pin_memory
            collate_fn=collate  # ä½¿ç”¨mmcvçš„collateå‡½æ•°å¤„ç†DataContainer
        )
        engine = None
        use_deepspeed = False
    
    # 7. æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯
    print(f"ğŸ“Š è®­ç»ƒä¿¡æ¯:")
    print(f"   - é…ç½®æ–‡ä»¶: {args.config}")
    print(f"   - å·¥ä½œç›®å½•: {cfg.work_dir}")
    print(f"   - è®¾å¤‡: {device_name}")
    print(f"   - ä¸–ç•Œå¤§å°: {world_size}")
    print(f"   - æœ¬åœ°rank: {local_rank}")
    print(f"   - è®­ç»ƒæ­¥æ•°: {args.steps}")
    
    # 8. å¼€å§‹è®­ç»ƒ - ä½¿ç”¨DeepSpeedä¼˜åŒ–çš„è®­ç»ƒå¾ªç¯
    try:
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        print("=" * 60)
        
        # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è®­ç»ƒå¾ªç¯ - DeepSpeedè¿”å›çš„train_dataloaderå¯ä»¥ç›´æ¥è¿­ä»£
        for step, batch in enumerate(train_dataloader):
            if step >= args.steps:
                break
                
            try:
                # æ£€æŸ¥æ‰¹æ¬¡æ•°æ®çš„æœ‰æ•ˆæ€§
                if batch is None:
                    print(f"âš ï¸ Step {step}: batchä¸ºNoneï¼Œè·³è¿‡")
                    continue
                    
                # DeepSpeedçš„DataLoaderä¼šè‡ªåŠ¨å¤„ç†æ•°æ®åˆ°è®¾å¤‡çš„ç§»åŠ¨
                # ä½†å¯¹äºXLAåç«¯ï¼Œæ‰‹åŠ¨ç¡®è®¤ä¸€ä¸‹æ›´ä¿é™©
                if use_deepspeed:
                    # å®‰å…¨åœ°è·å–inputså’Œdata_samples
                    if isinstance(batch, dict):
                        inputs = batch.get('inputs')
                        data_samples = batch.get('data_samples', [])
                    else:
                        print(f"âš ï¸ Step {step}: æ„å¤–çš„batchæ ¼å¼: {type(batch)}")
                        continue
                    
                    if inputs is None:
                        print(f"âš ï¸ Step {step}: inputsä¸ºNoneï¼Œè·³è¿‡")
                        continue
                    
                    inputs = inputs.to(engine.device)
                    if data_samples:
                        data_samples = [s.to(engine.device) for s in data_samples]
                    
                    # å‰å‘ä¼ æ’­
                    outputs = engine(inputs, data_samples, mode='loss')
                    if isinstance(outputs, dict) and 'loss' in outputs:
                        loss = outputs['loss']
                    else:
                        print(f"âš ï¸ Step {step}: æ— æ³•è·å–lossï¼Œoutputs: {type(outputs)}")
                        continue
                    
                    # æ‰“å°ä¿¡æ¯
                    print(f"[{local_rank}] step={step} loss={loss.item():.6f} device={loss.device}")
                    
                    # åå‘ä¼ æ’­å’Œæ›´æ–°
                    engine.backward(loss)
                    engine.step()
                    print(f"[{local_rank}] step={step} backward+step âœ…")
                else:
                    # å•GPUæ¨¡å¼å›é€€å¤„ç†
                    if isinstance(batch, dict):
                        inputs = batch.get('inputs', batch.get('img'))
                        data_samples = batch.get('data_samples', [])
                        
                        if inputs is not None:
                            inputs = inputs.to(device_name)
                    else:
                        inputs = batch[0].to(device_name) if len(batch) > 0 else None
                        data_samples = batch[1] if len(batch) > 1 else []
                    
                    if inputs is None:
                        print(f"âš ï¸ Step {step}: inputsä¸ºNoneï¼Œè·³è¿‡")
                        continue
                    
                    optimizer.zero_grad()
                    outputs = model(inputs, data_samples, mode='loss')
                    loss = outputs['loss'] if isinstance(outputs, dict) else outputs
                    
                    print(f"[{local_rank}] step={step} loss={loss.item():.6f} device={loss.device}")
                    
                    loss.backward()
                    optimizer.step()
                    print(f"[{local_rank}] step={step} backward+step âœ…")
                
                # æ·»åŠ all-reduceæµ‹è¯•ï¼ˆä»…åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼‰
                if torch.distributed.is_initialized():
                    test_tensor = torch.tensor([local_rank + 1.0], device=device_name)
                    torch.distributed.all_reduce(test_tensor, op=torch.distributed.ReduceOp.SUM)
                    expected_sum = sum(range(world_size)) + world_size
                    print(f"[{local_rank}] all_reduce sum result: {test_tensor.item()} (should be {expected_sum})")
                else:
                    print(f"[{local_rank}] åˆ†å¸ƒå¼ç¯å¢ƒæœªåˆå§‹åŒ–ï¼Œè·³è¿‡all_reduceæµ‹è¯•")
                
                # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œä¸æˆåŠŸdemoä¿æŒä¸€è‡´
                time.sleep(0.5)
                
            except Exception as step_error:
                print(f"âŒ Step {step} è®­ç»ƒå‡ºé”™: {step_error}")
                print(f"âŒ é”™è¯¯ç±»å‹: {type(step_error).__name__}")
                print(f"âŒ é”™è¯¯è¯¦æƒ…: {str(step_error)}")
                
                # å¦‚æœæ˜¯å…³é”®é”™è¯¯ï¼Œåœæ­¢è®­ç»ƒ
                if isinstance(step_error, (RuntimeError, KeyError, AttributeError)):
                    print("ğŸ’¥ é‡åˆ°å…³é”®é”™è¯¯ï¼Œåœæ­¢è®­ç»ƒ")
                    break
                else:
                    print("âš ï¸ éå…³é”®é”™è¯¯ï¼Œç»§ç»­è®­ç»ƒ")
                    continue
        
        # è®¡ç®—è®­ç»ƒæ—¶é—´
        end_time = time.time()
        training_time = end_time - start_time
        
        print("=" * 60)
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’ ({training_time/3600:.2f}å°æ—¶)")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {cfg.work_dir}")
        
        # ä¿å­˜æ¨¡å‹
        if local_rank == 0:
            save_path = f"{cfg.work_dir}/final_model.pth"
            if use_deepspeed and engine is not None:
                torch.save(engine.module.state_dict(), save_path)
            else:
                torch.save(model.state_dict(), save_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")
        
        print("ğŸ‰ è„šæœ¬æ‰§è¡Œå®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()