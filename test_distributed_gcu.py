#!/usr/bin/env python3
"""
ç‡§åŸT20 8å¡åˆ†å¸ƒå¼è¿æ¥æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯åˆ†å¸ƒå¼ç¯å¢ƒæ˜¯å¦æ­£å¸¸å·¥ä½œ
åŸºäºç‡§åŸå®˜æ–¹æœ€ä½³å®è·µ
"""

import os
import sys
import torch

# ğŸ”§ è®¾ç½®ç‡§åŸGCUç¯å¢ƒå˜é‡ - åŸºäºå®˜æ–¹æœ€ä½³å®è·µ
os.environ.setdefault('ENFLAME_CLUSTER_PARALLEL', 'true')
os.environ.setdefault('ENFLAME_ENABLE_EFP', 'true')
os.environ.setdefault('TOPS_VISIBLE_DEVICES', '0,1,2,3,4,5,6,7')
os.environ.setdefault('OMP_NUM_THREADS', '5')
os.environ.setdefault('ECCL_ASYNC_DISABLE', 'false')
os.environ.setdefault('ENABLE_RDMA', 'true')
os.environ.setdefault('ECCL_MAX_NCHANNELS', '2')
os.environ.setdefault('ENFLAME_UMD_FLAGS', 'mem_alloc_retry_times=1')
os.environ.setdefault('ECCL_RUNTIME_3_0_ENABLE', 'true')
os.environ.setdefault('ENFLAME_PT_EVALUATE_TENSOR_NEEDED', 'false')

# ç¦ç”¨CUDA
os.environ['CUDA_VISIBLE_DEVICES'] = ''

try:
    import torch_gcu
    torch_gcu_available = True
except ImportError:
    torch_gcu_available = False

def test_distributed():
    """æµ‹è¯•åˆ†å¸ƒå¼è¿æ¥"""
    
    print("ğŸ”§ æµ‹è¯•ç‡§åŸT20 8å¡åˆ†å¸ƒå¼è¿æ¥...")
    
    # è·å–åˆ†å¸ƒå¼å‚æ•°
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    rank = int(os.environ.get('RANK', 0))
    
    print(f"ğŸ”§ åˆ†å¸ƒå¼å‚æ•°: local_rank={local_rank}, world_size={world_size}, rank={rank}")
    
    # æ£€æŸ¥GCUè®¾å¤‡
    if torch_gcu_available:
        try:
            device_count = torch_gcu.device_count()
            print(f"ğŸ” æ£€æµ‹åˆ° {device_count} ä¸ª GCU è®¾å¤‡")
            
            # è®¾ç½®å½“å‰è®¾å¤‡
            torch_gcu.set_device(local_rank)
            current_device = torch_gcu.current_device()
            print(f"ğŸ”§ å½“å‰è¿›ç¨‹ä½¿ç”¨ GCU è®¾å¤‡: {current_device}")
            
            # åˆ›å»ºæµ‹è¯•å¼ é‡
            test_tensor = torch.randn(4, 4).to(f'gcu:{current_device}')
            print(f"âœ… æˆåŠŸåœ¨ GCU:{current_device} ä¸Šåˆ›å»ºå¼ é‡: {test_tensor.shape}")
            
        except Exception as e:
            print(f"âš ï¸ GCU æ“ä½œå¤±è´¥: {e}")
            return False
    else:
        print("âš ï¸ torch_gcu ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
    
    # æµ‹è¯•åˆ†å¸ƒå¼é€šä¿¡
    if world_size > 1:
        try:
            print("ğŸ”§ åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„...")
            torch.distributed.init_process_group(
                backend='eccl',
                init_method='env://',
                world_size=world_size,
                rank=rank
            )
            print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆ›å»ºæµ‹è¯•å¼ é‡è¿›è¡Œall_reduce
            if torch_gcu_available:
                test_tensor = torch.ones(2, 2).to(f'gcu:{local_rank}') * rank
            else:
                test_tensor = torch.ones(2, 2) * rank
            
            print(f"Rank {rank} åŸå§‹å¼ é‡: {test_tensor}")
            
            # æ‰§è¡Œall_reduce
            torch.distributed.all_reduce(test_tensor)
            print(f"Rank {rank} all_reduceåå¼ é‡: {test_tensor}")
            
            torch.distributed.barrier()
            print(f"âœ… Rank {rank} åˆ†å¸ƒå¼é€šä¿¡æµ‹è¯•æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ Rank {rank} åˆ†å¸ƒå¼é€šä¿¡å¤±è´¥: {e}")
            return False
    else:
        print("ğŸ”§ å•å¡æ¨¡å¼ï¼Œè·³è¿‡åˆ†å¸ƒå¼é€šä¿¡æµ‹è¯•")
    
    print(f"âœ… Rank {rank} æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    return True

if __name__ == '__main__':
    success = test_distributed()
    sys.exit(0 if success else 1)
